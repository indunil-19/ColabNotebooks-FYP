{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indunil-19/ColabNotebooks-FYP/blob/main/Adapter_based_fine_tuning_without_trainer-8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoL8-pXx9z7c"
      },
      "source": [
        "## Training Single Task Adapters\n",
        "For Bert SinBert and XLM-R Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KSLx_mnliOvH",
        "outputId": "18155906-11f9-4927-a4bb-de7663e94336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device name\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "import torch\n",
        "print(\"GPU Device name\")\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qGOfIYJ3Tnm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd072a65-5ddf-4e52-8e10-c1e1f908362d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCdvw5JNlygg"
      },
      "source": [
        "### **Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6LjxhVXklun6"
      },
      "outputs": [],
      "source": [
        "technique = \"Sentiment\"\n",
        "experiment_no = \"1\"\n",
        "oversample_dataset = False\n",
        "over_sampling_technique = \"ROS\"\n",
        "sampling_strategy = \"1:0.25:0.25\"\n",
        "validation_size = (1/9)\n",
        "test_size = 0.1\n",
        "split_random_state = 42\n",
        "training_seed = 42 #@param [ 8, 42, 77]\n",
        "NO_OUTPUT_LAYERS = 4\n",
        "id2label={ 0: \"Negative\", 1: \"Neutral\", 2: \"Positive\", 3:\"Conflict\"}\n",
        "tag_set = [\"Positive\", \"Negative\", \"Neutral\", \"Conflict\"]\n",
        "script=\"Char-Script-1.0\"\n",
        "\n",
        "\n",
        "load_adapter = False #@param {type:\"boolean\"}\n",
        "unfreeze_model = False #@param {type:\"boolean\"}\n",
        "save_adapter = False #@param {type:\"boolean\"}\n",
        "lang_adapter_setting = \"parallel\" #@param [\"none\", \"stack\", \"parallel\"]\n",
        "adapter_config = \"houlsby\" #@param [\"houlsby\", \"pfeiffer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Z5zzz0_5IxVc"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 5e-4\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "KanOOHjzWI7b"
      },
      "outputs": [],
      "source": [
        "# technique=\"hate\"\n",
        "pretrained_adapter_path = \"/content/drive/Shareddrives/Lingua/Final/\"+technique #+ \"_\" + str(random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9gYpCV28OA"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "qL3Sq1HQynCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d0e6ae-e926-4d01-cf16-b38df6b144f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adapter-transformers==3.1.0 in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers==3.1.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter-transformers==3.1.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter-transformers==3.1.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers==3.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers==3.1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers==3.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers==3.1.0) (2023.7.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U adapter-transformers==3.1.0\n",
        "!pip install datasets\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUHpDE_Gtyen"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "mUcFdH0HIxVe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "xJZ6z8bJl25l"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "# import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "import io\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoAdapterModel ,AutoTokenizer, AdapterConfig,AutoConfig, AutoModelWithHeads, TrainingArguments, EvalPrediction, AdamW, get_scheduler, TextClassificationPipeline, EarlyStoppingCallback,set_seed\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup,set_seed\n",
        "from transformers.adapters.composition import Fuse, Stack, Parallel\n",
        "from datasets import load_metric\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzrDM6Ua-jo_"
      },
      "source": [
        "### Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "BB8pw1RDuDaA"
      },
      "outputs": [],
      "source": [
        "def apply_oversampling(x, y):\n",
        "\n",
        "  (unique, counts) = np.unique(y, axis=0, return_counts=True)\n",
        "  print(\"Class Distribution Without Oversampling\", counts)\n",
        "\n",
        "  # define oversampling strategy\n",
        "  if (over_sampling_technique == \"\"):\n",
        "    return x, y\n",
        "  elif (over_sampling_technique == \"ROS\"):\n",
        "    if (technique==\"humor\"):\n",
        "      oversample = RandomOverSampler(sampling_strategy = float(sampling_strategy))\n",
        "    else:\n",
        "      sampling_ratio = sampling_strategy.split(\":\");\n",
        "      oversample = RandomOverSampler(sampling_strategy = {\n",
        "          0:int(counts[0]*float(sampling_ratio[0])),\n",
        "          1:int(counts[0]*float(sampling_ratio[1])),\n",
        "          2:int(counts[0]*float(sampling_ratio[2]))\n",
        "          })\n",
        "  elif (over_sampling_technique == \"ADASYN\"):\n",
        "    oversample = ADASYN(sampling_strategy=\"minority\")\n",
        "  elif (over_sampling_technique == \"SMOTE\"):\n",
        "    oversample = SMOTE()\n",
        "  elif (over_sampling_technique == \"BorderlineSMOTE\"):\n",
        "    oversample = BorderlineSMOTE()\n",
        "\n",
        "  # fit and apply the transform\n",
        "  X_over, y_over = oversample.fit_resample(x, y)\n",
        "\n",
        "  (unique, counts) = np.unique(y_over, axis=0, return_counts=True)\n",
        "  print(\"Class Distribution After Oversampling\", counts)\n",
        "\n",
        "  return X_over, y_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "0MUpioPlmvMh"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/Shareddrives/Lingua/Sinhala-English CMCS Dataset/annotated-script(all).csv\"\n",
        "model_save_path = \"/kaggle/working/\"+technique+\"/\"+experiment_no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yq5AMmjelqlj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path)\n",
        "df = df[['Sentence', technique, script]]\n",
        "df.columns = ['Sentence', 'Label', script]\n",
        "\n",
        "df['Label'], uniq = pd.factorize(df['Label'])\n",
        "\n",
        "X, y = df[['Sentence', script]], df[['Label']]\n",
        "stratifying_col = y[\"Label\"]\n",
        "X_rem, X_test, y_rem, y_test = train_test_split(X, y, test_size=test_size, stratify=stratifying_col, random_state=split_random_state)\n",
        "stratifying_col = y_rem[\"Label\"]\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_rem, y_rem, test_size=validation_size, stratify=stratifying_col, random_state=split_random_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Ppy0Sj5NIxVg"
      },
      "outputs": [],
      "source": [
        "del df, X, y, stratifying_col, X_rem, y_rem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "oHELisZ0ufPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8d4528-f2f8-4731-8a95-b71bff525a69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Negative', 'Neutral', 'Positive', 'Conflict'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "uniq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "dkRs6oJvIxVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5b2abb-8a67-4db1-87e1-c22a35114251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train : Rows = 10814 , Columns =  2\n",
            "y_train : Rows = 10814 , Columns =  1\n",
            "X_validation : Rows = 1352 , Columns =  2\n",
            "y_validation : Rows = 1352 , Columns =  1\n",
            "X_test : Rows = 1352 , Columns =  2\n",
            "y_test : Rows = 1352 , Columns =  1\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train : Rows =\", X_train.shape[0], \", Columns = \", X_train.shape[1])\n",
        "print(\"y_train : Rows =\", y_train.shape[0], \", Columns = \", y_train.shape[1])\n",
        "print(\"X_validation : Rows =\", X_validation.shape[0], \", Columns = \", X_validation.shape[1])\n",
        "print(\"y_validation : Rows =\", y_validation.shape[0], \", Columns = \", y_validation.shape[1])\n",
        "print(\"X_test : Rows =\", X_test.shape[0], \", Columns = \", X_test.shape[1])\n",
        "print(\"y_test : Rows =\", y_test.shape[0], \", Columns = \", y_test.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "UaUlvhRwIxVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afe4cdc-6442-4b58-ad6c-484a5f9717f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels : ['Negative', 'Neutral', 'Positive', 'Conflict']\n",
            "Train : [2913, 6877, 924, 100]\n",
            "Validation : [364, 860, 116, 12]\n",
            "Test : [364, 860, 116, 12]\n"
          ]
        }
      ],
      "source": [
        "print(\"Labels :\", ['Negative', 'Neutral', 'Positive', 'Conflict'])\n",
        "print(\"Train :\", y_train.groupby('Label').size().tolist())\n",
        "print(\"Validation :\", y_validation.groupby('Label').size().tolist())\n",
        "print(\"Test :\", y_test.groupby('Label').size().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "7tCiiAeuIxVi"
      },
      "outputs": [],
      "source": [
        "if oversample_dataset:\n",
        "  X_train = np.array(X_train).reshape(-1, 1)\n",
        "  X_train, y_train = apply_oversampling(X_train, y_train)\n",
        "  X_train = [x[0] for x in X_train.tolist()]\n",
        "# y_train = y_train.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw-aKg9RIxVi"
      },
      "source": [
        "##### **Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "T_2iifBVIxVi"
      },
      "outputs": [],
      "source": [
        "set_seed(training_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "SxzzTXwZIxVj"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = X_train.values.tolist(), y_train.values.tolist()\n",
        "X_validation, y_validation = X_validation.values.tolist(), y_validation.values.tolist()\n",
        "X_test, y_test = X_test.values.tolist(), y_test.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "oQbG0JmQIxVj"
      },
      "outputs": [],
      "source": [
        "X_train_arr=[]\n",
        "y_train_arr=[]\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    X_train_arr.append(X_train[i][0])\n",
        "    y_train_arr.append(y_train[i][0])\n",
        "\n",
        "\n",
        "X_validation_arr=[]\n",
        "y_validation_arr=[]\n",
        "for i in range(len(X_validation)):\n",
        "    X_validation_arr.append(X_validation[i][0])\n",
        "    y_validation_arr.append(y_validation[i][0])\n",
        "\n",
        "\n",
        "X_test_arr=[]\n",
        "y_test_arr=[]\n",
        "X_test_latin=[]\n",
        "y_test_latin=[]\n",
        "\n",
        "X_test_Sinhala=[]\n",
        "y_test_Sinhala=[]\n",
        "\n",
        "X_test_Mixed=[]\n",
        "y_test_Mixed=[]\n",
        "for i in range(len(X_test)):\n",
        "    X_test_arr.append(X_test[i][0])\n",
        "    y_test_arr.append(y_test[i][0])\n",
        "\n",
        "    if X_test[i][1]==\"Latin\":\n",
        "        X_test_latin.append(X_test[i][0])\n",
        "        y_test_latin.append(y_test[i][0])\n",
        "\n",
        "    elif X_test[i][1]==\"Sinhala\":\n",
        "        X_test_Sinhala.append(X_test[i][0])\n",
        "        y_test_Sinhala.append(y_test[i][0])\n",
        "\n",
        "    elif X_test[i][1]==\"Mixed\":\n",
        "        X_test_Mixed.append(X_test[i][0])\n",
        "        y_test_Mixed.append(y_test[i][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "lCUWfe8-n0i8"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "TqXJtJe8oObQ"
      },
      "outputs": [],
      "source": [
        "encoded_X_train = tokenizer(X_train_arr, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "encoded_X_validation = tokenizer(X_validation_arr, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "encoded_X_test = tokenizer(X_test_arr, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "encoded_X_test_latin = tokenizer(X_test_latin, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "encoded_X_test_Sinhala = tokenizer(X_test_Sinhala, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "encoded_X_test_Mixed = tokenizer(X_test_Mixed, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "6581Al-K_Nyf"
      },
      "outputs": [],
      "source": [
        "class DatasetObject(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = DatasetObject(encoded_X_train, y_train_arr)\n",
        "validation_dataset = DatasetObject(encoded_X_validation, y_validation_arr)\n",
        "test_dataset = DatasetObject(encoded_X_test, y_test_arr)\n",
        "test_dataset_latin = DatasetObject(encoded_X_test_latin, y_test_latin)\n",
        "test_dataset_Sinhala = DatasetObject(encoded_X_test_Sinhala, y_test_Sinhala)\n",
        "test_dataset_Mixed = DatasetObject(encoded_X_test_Mixed, y_test_Mixed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Mol-S8fDIxVk"
      },
      "outputs": [],
      "source": [
        "# train_sampler = RandomSampler(train_dataset)\n",
        "train_sampler = SequentialSampler(train_dataset)\n",
        "validation_sampler = SequentialSampler(validation_dataset)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "validation_sampler_latin = SequentialSampler(test_dataset_latin)\n",
        "validation_sampler_sinhala= SequentialSampler(test_dataset_Sinhala)\n",
        "validation_sampler_mixed = SequentialSampler(test_dataset_Mixed)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, sampler=train_sampler , batch_size=BATCH_SIZE)\n",
        "validation_loader = DataLoader(validation_dataset, sampler=validation_sampler , batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, sampler=test_sampler , batch_size=BATCH_SIZE)\n",
        "test_loader_latin = DataLoader(test_dataset_latin, sampler=validation_sampler_latin , batch_size=BATCH_SIZE)\n",
        "test_loader_Sinhala = DataLoader(test_dataset_Sinhala, sampler=validation_sampler_sinhala , batch_size=BATCH_SIZE)\n",
        "test_loader_Mixed = DataLoader(test_dataset_Mixed, sampler=validation_sampler_mixed , batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCwk6iQE_XZb"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "KNyi1nFH_TOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d097b3-9136-45f7-e0af-0888f5f31279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/adapters/models/roberta/adapter_model.py:255: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/adapters/models/roberta/adapter_model.py:233: FutureWarning: This class has been renamed to `RobertaModelWithHeads` in v3. Please use the new class instead as this class might be removed in a future version.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaModelWithHeads were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "config = AutoConfig.from_pretrained(\"xlm-roberta-base\", num_labels=NO_OUTPUT_LAYERS)\n",
        "model = AutoModelWithHeads.from_pretrained(\"xlm-roberta-base\", config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "7RPhSipb_diC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94de80d7-2d88-43b4-b49c-4b2b24e493ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adding new adapter houlsby\n"
          ]
        }
      ],
      "source": [
        "# Load an adapter\n",
        "if load_adapter:\n",
        "  print(\"loading adapter from\", pretrained_adapter_path)\n",
        "  model.load_adapter(pretrained_adapter_path, with_head=False)\n",
        "\n",
        "# Add a new adapter\n",
        "else:\n",
        "  print(\"adding new adapter\", adapter_config)\n",
        "  if adapter_config == \"pfeiffer\":\n",
        "    config = AdapterConfig.load(\"pfeiffer\", reduction_factor=12)\n",
        "  else:\n",
        "    config = AdapterConfig.load(\"houlsby\")\n",
        "  model.add_adapter(\"task_\"+technique, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Zq1dG4sVcDX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8bc15f3-69cf-4d68-d7e2-054c158f0f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stacking parallel language adapters set\n"
          ]
        }
      ],
      "source": [
        "# Add a classification head\n",
        "model.add_classification_head(\n",
        "  \"task_\"+technique,\n",
        "  num_labels=NO_OUTPUT_LAYERS,\n",
        "  id2label=id2label\n",
        ")\n",
        "\n",
        "# Without Language Adapters\n",
        "if lang_adapter_setting == \"none\":\n",
        "  model.set_active_adapters(\"task_\"+technique)\n",
        "\n",
        "else:\n",
        "  # Load language adapters\n",
        "  lang_adapter_config = AdapterConfig.load(\"pfeiffer+inv\")\n",
        "  model.load_adapter(\"/content/drive/Shareddrives/Lingua/adapters/lan/mlm\", config=lang_adapter_config, load_as=\"si-en\", with_head=False)\n",
        "  # model.load_adapter(\"/content/drive/Shareddrives/FYP/TrainedAdapters/si_mlm\", config=lang_adapter_config, load_as=\"si\", with_head=False)\n",
        "  model.load_adapter(\"/content/drive/Shareddrives/Lingua/adapters/lan/si_mlm\", config=lang_adapter_config, load_as=\"si\", with_head=False)\n",
        "  config = AdapterConfig.load(\"pfeiffer\", non_linearity=\"relu\", reduction_factor=2)\n",
        "  model.load_adapter(\"en/wiki@ukp\", config=config)\n",
        "\n",
        "  # Stack Language Adapters\n",
        "  if lang_adapter_setting == \"stack\":\n",
        "    print(\"stacking language adapters\")\n",
        "    model.set_active_adapters(Stack(\"en\", \"si\", \"si-en\", \"task_\"+technique))\n",
        "\n",
        "  # Parallel Language Adapters\n",
        "  else:\n",
        "    print(\"stacking parallel language adapters set\")\n",
        "    model.set_active_adapters(Stack(Parallel(\"en\", \"si\", \"si-en\"), \"task_\"+technique))\n",
        "\n",
        "# Train Adapter\n",
        "model.train_adapter(\"task_\"+technique)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "iVTWkQaiiuhZ"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the model to train both the model and adapter\n",
        "if unfreeze_model:\n",
        "  model.freeze_model(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "SBMtmX77IMlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b26186-c89d-4a93-db65-a0f0a7d01770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "model.cuda()\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "bny18Y2qHvf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "975c63d6-e74e-4507-eb89-e78ebc43973e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Iu1hV1g3jAvS"
      },
      "outputs": [],
      "source": [
        "set_seed(training_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "ZDiEfZumNwYA"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(allpreds,alllabels):\n",
        "    metric1 = load_metric(\"precision\")\n",
        "    metric2 = load_metric(\"recall\")\n",
        "    metric3 = load_metric(\"f1\")\n",
        "    metric4 = load_metric(\"accuracy\")\n",
        "\n",
        "    predictions, labels = allpreds,alllabels\n",
        "    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
        "    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
        "    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    macro_precision = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
        "    macro_recall = metric2.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
        "    macro_f1 = metric3.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "    return {\"accuracy\":accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"macro_precision\": macro_precision, \"macro_recall\": macro_recall, \"macro_f1\": macro_f1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ErqMNvyEGL07"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        allpreds.extend(torch.argmax(logits, dim=-1))\n",
        "        alllabels.extend(batch[\"labels\"])\n",
        "    return compute_metrics(allpreds,alllabels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "_oEcPkiDGOjQ"
      },
      "outputs": [],
      "source": [
        "def calculate_loss_and_f1(model, dataloader):\n",
        "    model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        allpreds.extend(torch.argmax(logits, dim=-1))\n",
        "        alllabels.extend(batch[\"labels\"])\n",
        "\n",
        "    macro_f1 = load_metric(\"f1\").compute(predictions=allpreds, references=alllabels, average=\"macro\")[\"f1\"]\n",
        "    return macro_f1, (total_loss/len(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "oesjvk80GQZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabb9bd1-c1c2-40fc-9f74-7316dbaeec03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "num_training_steps = EPOCHS * len(train_loader)\n",
        "betas = (0.9, 0.999)\n",
        "eps = 1e-08\n",
        "num_warmup_steps = 0\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE,betas=betas,eps=eps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "5l6csY77GS8F"
      },
      "outputs": [],
      "source": [
        "tot_loss = 0\n",
        "log_loss = 0\n",
        "best_val_acc = 0\n",
        "\n",
        "tot_train_time = 0\n",
        "pbar_update_freq = 10\n",
        "\n",
        "glb_step = 0\n",
        "actual_step = 0\n",
        "max_grad_norm = 1.0\n",
        "eval_every_steps = 100\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "leave_training = False\n",
        "val_metric = \"macro_f1\"\n",
        "\n",
        "best_epoch = -1\n",
        "early_stop_epoch_thresh = 5\n",
        "\n",
        "epoch_traces = []\n",
        "acc_traces = []\n",
        "validation_loss_traces = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "dkv8HsFgGVGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6239876-e4d8-4923-9ff5-8340f85a04a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:   0%|          | 0/6760 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:  35%|███▍      | 2360/6760 [28:50<53:45,  1.36it/s, Average Loss=0.0942, Epoch=6]\n",
            "\n",
            "Train:   0%|          | 10/6760 [00:04<53:15,  2.11it/s]\u001b[A\n",
            "Train:   0%|          | 10/6760 [00:04<53:15,  2.11it/s, Average Loss=1.04, Epoch=0]\u001b[A\n",
            "Train:   0%|          | 20/6760 [00:09<53:42,  2.09it/s, Average Loss=1.04, Epoch=0]\u001b[A\n",
            "Train:   0%|          | 20/6760 [00:09<53:42,  2.09it/s, Average Loss=0.898, Epoch=0]\u001b[A\n",
            "Train:   0%|          | 30/6760 [00:14<55:22,  2.03it/s, Average Loss=0.898, Epoch=0]\u001b[A\n",
            "Train:   0%|          | 30/6760 [00:14<55:22,  2.03it/s, Average Loss=0.936, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 40/6760 [00:20<57:15,  1.96it/s, Average Loss=0.936, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 40/6760 [00:20<57:15,  1.96it/s, Average Loss=0.881, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 50/6760 [00:24<56:04,  1.99it/s, Average Loss=0.881, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 50/6760 [00:24<56:04,  1.99it/s, Average Loss=0.897, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 60/6760 [00:29<55:05,  2.03it/s, Average Loss=0.897, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 60/6760 [00:29<55:05,  2.03it/s, Average Loss=0.867, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 70/6760 [00:34<55:55,  1.99it/s, Average Loss=0.867, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 70/6760 [00:34<55:55,  1.99it/s, Average Loss=0.914, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 80/6760 [00:39<54:10,  2.06it/s, Average Loss=0.914, Epoch=0]\u001b[A\n",
            "Train:   1%|          | 80/6760 [00:39<54:10,  2.06it/s, Average Loss=0.944, Epoch=0]\u001b[A\n",
            "Train:   1%|▏         | 90/6760 [00:43<52:42,  2.11it/s, Average Loss=0.944, Epoch=0]\u001b[A\n",
            "Train:   1%|▏         | 90/6760 [00:43<52:42,  2.11it/s, Average Loss=0.808, Epoch=0]\u001b[A\n",
            "Train:   1%|▏         | 100/6760 [00:48<51:42,  2.15it/s, Average Loss=0.808, Epoch=0]\u001b[A\n",
            "Train:   1%|▏         | 100/6760 [00:48<51:42,  2.15it/s, Average Loss=0.856, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 110/6760 [00:52<51:00,  2.17it/s, Average Loss=0.856, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 110/6760 [00:52<51:00,  2.17it/s, Average Loss=0.769, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 120/6760 [00:57<50:24,  2.20it/s, Average Loss=0.769, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 120/6760 [00:57<50:24,  2.20it/s, Average Loss=0.784, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 130/6760 [01:01<49:54,  2.21it/s, Average Loss=0.784, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 130/6760 [01:01<49:54,  2.21it/s, Average Loss=0.719, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 140/6760 [01:06<49:32,  2.23it/s, Average Loss=0.719, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 140/6760 [01:06<49:32,  2.23it/s, Average Loss=0.794, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 150/6760 [01:10<49:25,  2.23it/s, Average Loss=0.794, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 150/6760 [01:10<49:25,  2.23it/s, Average Loss=0.805, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 160/6760 [01:15<49:20,  2.23it/s, Average Loss=0.805, Epoch=0]\u001b[A\n",
            "Train:   2%|▏         | 160/6760 [01:15<49:20,  2.23it/s, Average Loss=0.845, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 170/6760 [01:19<49:14,  2.23it/s, Average Loss=0.845, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 170/6760 [01:19<49:14,  2.23it/s, Average Loss=0.738, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 180/6760 [01:24<49:15,  2.23it/s, Average Loss=0.738, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 180/6760 [01:24<49:15,  2.23it/s, Average Loss=0.687, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 190/6760 [01:28<49:23,  2.22it/s, Average Loss=0.687, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 190/6760 [01:28<49:23,  2.22it/s, Average Loss=0.69, Epoch=0] \u001b[A\n",
            "Train:   3%|▎         | 200/6760 [01:33<49:27,  2.21it/s, Average Loss=0.69, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 200/6760 [01:33<49:27,  2.21it/s, Average Loss=0.718, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 210/6760 [01:37<49:25,  2.21it/s, Average Loss=0.718, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 210/6760 [01:37<49:25,  2.21it/s, Average Loss=0.728, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 220/6760 [01:42<49:29,  2.20it/s, Average Loss=0.728, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 220/6760 [01:42<49:29,  2.20it/s, Average Loss=0.724, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 230/6760 [01:46<49:36,  2.19it/s, Average Loss=0.724, Epoch=0]\u001b[A\n",
            "Train:   3%|▎         | 230/6760 [01:46<49:36,  2.19it/s, Average Loss=0.713, Epoch=0]\u001b[A\n",
            "Train:   4%|▎         | 240/6760 [01:51<49:29,  2.20it/s, Average Loss=0.713, Epoch=0]\u001b[A\n",
            "Train:   4%|▎         | 240/6760 [01:51<49:29,  2.20it/s, Average Loss=0.629, Epoch=0]\u001b[A\n",
            "Train:   4%|▎         | 250/6760 [01:55<49:23,  2.20it/s, Average Loss=0.629, Epoch=0]\u001b[A\n",
            "Train:   4%|▎         | 250/6760 [01:55<49:23,  2.20it/s, Average Loss=0.679, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 260/6760 [02:00<49:18,  2.20it/s, Average Loss=0.679, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 260/6760 [02:00<49:18,  2.20it/s, Average Loss=0.759, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 270/6760 [02:05<49:13,  2.20it/s, Average Loss=0.759, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 270/6760 [02:05<49:13,  2.20it/s, Average Loss=0.703, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 280/6760 [02:09<48:58,  2.21it/s, Average Loss=0.703, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 280/6760 [02:09<48:58,  2.21it/s, Average Loss=0.543, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 290/6760 [02:14<48:56,  2.20it/s, Average Loss=0.543, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 290/6760 [02:14<48:56,  2.20it/s, Average Loss=0.59, Epoch=0] \u001b[A\n",
            "Train:   4%|▍         | 300/6760 [02:18<48:49,  2.20it/s, Average Loss=0.59, Epoch=0]\u001b[A\n",
            "Train:   4%|▍         | 300/6760 [02:18<48:49,  2.20it/s, Average Loss=0.623, Epoch=0]\u001b[A\n",
            "Train:   5%|▍         | 310/6760 [02:23<49:09,  2.19it/s, Average Loss=0.623, Epoch=0]\u001b[A\n",
            "Train:   5%|▍         | 310/6760 [02:23<49:09,  2.19it/s, Average Loss=0.63, Epoch=0] \u001b[A\n",
            "Train:   5%|▍         | 320/6760 [02:27<48:59,  2.19it/s, Average Loss=0.63, Epoch=0]\u001b[A\n",
            "Train:   5%|▍         | 320/6760 [02:27<48:59,  2.19it/s, Average Loss=0.72, Epoch=0]\u001b[A\n",
            "Train:   5%|▍         | 330/6760 [02:32<48:36,  2.20it/s, Average Loss=0.72, Epoch=0]\u001b[A\n",
            "Train:   5%|▍         | 330/6760 [02:32<48:36,  2.20it/s, Average Loss=0.62, Epoch=0]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 0, Macro F1: 0.5019278013263578, Validation Loss: 0.668956304012343, Time per Epoch: 165.2103452682495]\n",
            "Begin Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:   5%|▌         | 340/6760 [02:51<1:33:56,  1.14it/s, Average Loss=0.62, Epoch=0]\u001b[A\n",
            "Train:   5%|▌         | 340/6760 [02:51<1:33:56,  1.14it/s, Average Loss=0.589, Epoch=1]\u001b[A\n",
            "Train:   5%|▌         | 350/6760 [02:55<1:19:09,  1.35it/s, Average Loss=0.589, Epoch=1]\u001b[A\n",
            "Train:   5%|▌         | 350/6760 [02:55<1:19:09,  1.35it/s, Average Loss=0.702, Epoch=1]\u001b[A\n",
            "Train:   5%|▌         | 360/6760 [02:59<1:08:51,  1.55it/s, Average Loss=0.702, Epoch=1]\u001b[A\n",
            "Train:   5%|▌         | 360/6760 [02:59<1:08:51,  1.55it/s, Average Loss=0.609, Epoch=1]\u001b[A\n",
            "Train:   5%|▌         | 370/6760 [03:03<1:01:31,  1.73it/s, Average Loss=0.609, Epoch=1]\u001b[A\n",
            "Train:   5%|▌         | 370/6760 [03:03<1:01:31,  1.73it/s, Average Loss=0.573, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 380/6760 [03:07<56:29,  1.88it/s, Average Loss=0.573, Epoch=1]  \u001b[A\n",
            "Train:   6%|▌         | 380/6760 [03:07<56:29,  1.88it/s, Average Loss=0.619, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 390/6760 [03:12<53:05,  2.00it/s, Average Loss=0.619, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 390/6760 [03:12<53:05,  2.00it/s, Average Loss=0.575, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 400/6760 [03:16<50:43,  2.09it/s, Average Loss=0.575, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 400/6760 [03:16<50:43,  2.09it/s, Average Loss=0.561, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 410/6760 [03:20<48:58,  2.16it/s, Average Loss=0.561, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 410/6760 [03:20<48:58,  2.16it/s, Average Loss=0.637, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 420/6760 [03:24<47:40,  2.22it/s, Average Loss=0.637, Epoch=1]\u001b[A\n",
            "Train:   6%|▌         | 420/6760 [03:24<47:40,  2.22it/s, Average Loss=0.581, Epoch=1]\u001b[A\n",
            "Train:   6%|▋         | 430/6760 [03:29<46:49,  2.25it/s, Average Loss=0.581, Epoch=1]\u001b[A\n",
            "Train:   6%|▋         | 430/6760 [03:29<46:49,  2.25it/s, Average Loss=0.553, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 440/6760 [03:33<46:15,  2.28it/s, Average Loss=0.553, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 440/6760 [03:33<46:15,  2.28it/s, Average Loss=0.51, Epoch=1] \u001b[A\n",
            "Train:   7%|▋         | 450/6760 [03:37<45:38,  2.30it/s, Average Loss=0.51, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 450/6760 [03:37<45:38,  2.30it/s, Average Loss=0.628, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 460/6760 [03:41<45:07,  2.33it/s, Average Loss=0.628, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 460/6760 [03:41<45:07,  2.33it/s, Average Loss=0.575, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 470/6760 [03:46<44:52,  2.34it/s, Average Loss=0.575, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 470/6760 [03:46<44:52,  2.34it/s, Average Loss=0.601, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 480/6760 [03:50<44:43,  2.34it/s, Average Loss=0.601, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 480/6760 [03:50<44:43,  2.34it/s, Average Loss=0.558, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 490/6760 [03:54<44:24,  2.35it/s, Average Loss=0.558, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 490/6760 [03:54<44:24,  2.35it/s, Average Loss=0.597, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 500/6760 [03:58<44:07,  2.36it/s, Average Loss=0.597, Epoch=1]\u001b[A\n",
            "Train:   7%|▋         | 500/6760 [03:58<44:07,  2.36it/s, Average Loss=0.561, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 510/6760 [04:02<44:01,  2.37it/s, Average Loss=0.561, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 510/6760 [04:03<44:01,  2.37it/s, Average Loss=0.563, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 520/6760 [04:07<43:59,  2.36it/s, Average Loss=0.563, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 520/6760 [04:07<43:59,  2.36it/s, Average Loss=0.523, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 530/6760 [04:11<43:52,  2.37it/s, Average Loss=0.523, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 530/6760 [04:11<43:52,  2.37it/s, Average Loss=0.52, Epoch=1] \u001b[A\n",
            "Train:   8%|▊         | 540/6760 [04:15<43:43,  2.37it/s, Average Loss=0.52, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 540/6760 [04:15<43:43,  2.37it/s, Average Loss=0.617, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 550/6760 [04:19<43:41,  2.37it/s, Average Loss=0.617, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 550/6760 [04:19<43:41,  2.37it/s, Average Loss=0.564, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 560/6760 [04:24<43:42,  2.36it/s, Average Loss=0.564, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 560/6760 [04:24<43:42,  2.36it/s, Average Loss=0.601, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 570/6760 [04:28<43:36,  2.37it/s, Average Loss=0.601, Epoch=1]\u001b[A\n",
            "Train:   8%|▊         | 570/6760 [04:28<43:36,  2.37it/s, Average Loss=0.537, Epoch=1]\u001b[A\n",
            "Train:   9%|▊         | 580/6760 [04:32<43:26,  2.37it/s, Average Loss=0.537, Epoch=1]\u001b[A\n",
            "Train:   9%|▊         | 580/6760 [04:32<43:26,  2.37it/s, Average Loss=0.501, Epoch=1]\u001b[A\n",
            "Train:   9%|▊         | 590/6760 [04:36<43:22,  2.37it/s, Average Loss=0.501, Epoch=1]\u001b[A\n",
            "Train:   9%|▊         | 590/6760 [04:36<43:22,  2.37it/s, Average Loss=0.539, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 600/6760 [04:41<43:26,  2.36it/s, Average Loss=0.539, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 600/6760 [04:41<43:26,  2.36it/s, Average Loss=0.569, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 610/6760 [04:45<43:21,  2.36it/s, Average Loss=0.569, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 610/6760 [04:45<43:21,  2.36it/s, Average Loss=0.491, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 620/6760 [04:49<43:13,  2.37it/s, Average Loss=0.491, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 620/6760 [04:49<43:13,  2.37it/s, Average Loss=0.43, Epoch=1] \u001b[A\n",
            "Train:   9%|▉         | 630/6760 [04:53<43:07,  2.37it/s, Average Loss=0.43, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 630/6760 [04:53<43:07,  2.37it/s, Average Loss=0.44, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 640/6760 [04:57<43:10,  2.36it/s, Average Loss=0.44, Epoch=1]\u001b[A\n",
            "Train:   9%|▉         | 640/6760 [04:57<43:10,  2.36it/s, Average Loss=0.494, Epoch=1]\u001b[A\n",
            "Train:  10%|▉         | 650/6760 [05:02<43:07,  2.36it/s, Average Loss=0.494, Epoch=1]\u001b[A\n",
            "Train:  10%|▉         | 650/6760 [05:02<43:07,  2.36it/s, Average Loss=0.573, Epoch=1]\u001b[A\n",
            "Train:  10%|▉         | 660/6760 [05:06<42:58,  2.37it/s, Average Loss=0.573, Epoch=1]\u001b[A\n",
            "Train:  10%|▉         | 660/6760 [05:06<42:58,  2.37it/s, Average Loss=0.581, Epoch=1]\u001b[A\n",
            "Train:  10%|▉         | 670/6760 [05:10<42:51,  2.37it/s, Average Loss=0.581, Epoch=1]\u001b[A\n",
            "Train:  10%|▉         | 670/6760 [05:10<42:51,  2.37it/s, Average Loss=0.5, Epoch=1]  \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 1, Macro F1: 0.5110145620748299, Validation Loss: 0.6169580921184185, Time per Epoch: 152.28039169311523]\n",
            "Begin Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  10%|█         | 680/6760 [05:29<1:26:30,  1.17it/s, Average Loss=0.5, Epoch=1]\u001b[A\n",
            "Train:  10%|█         | 680/6760 [05:29<1:26:30,  1.17it/s, Average Loss=0.483, Epoch=2]\u001b[A\n",
            "Train:  10%|█         | 690/6760 [05:33<1:13:18,  1.38it/s, Average Loss=0.483, Epoch=2]\u001b[A\n",
            "Train:  10%|█         | 690/6760 [05:33<1:13:18,  1.38it/s, Average Loss=0.64, Epoch=2] \u001b[A\n",
            "Train:  10%|█         | 700/6760 [05:37<1:03:59,  1.58it/s, Average Loss=0.64, Epoch=2]\u001b[A\n",
            "Train:  10%|█         | 700/6760 [05:37<1:03:59,  1.58it/s, Average Loss=0.464, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 710/6760 [05:41<57:26,  1.76it/s, Average Loss=0.464, Epoch=2]  \u001b[A\n",
            "Train:  11%|█         | 710/6760 [05:41<57:26,  1.76it/s, Average Loss=0.434, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 720/6760 [05:46<52:56,  1.90it/s, Average Loss=0.434, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 720/6760 [05:46<52:56,  1.90it/s, Average Loss=0.541, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 730/6760 [05:50<49:53,  2.01it/s, Average Loss=0.541, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 730/6760 [05:50<49:53,  2.01it/s, Average Loss=0.456, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 740/6760 [05:54<47:41,  2.10it/s, Average Loss=0.456, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 740/6760 [05:54<47:41,  2.10it/s, Average Loss=0.47, Epoch=2] \u001b[A\n",
            "Train:  11%|█         | 750/6760 [05:58<46:04,  2.17it/s, Average Loss=0.47, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 750/6760 [05:58<46:04,  2.17it/s, Average Loss=0.537, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 760/6760 [06:03<44:56,  2.23it/s, Average Loss=0.537, Epoch=2]\u001b[A\n",
            "Train:  11%|█         | 760/6760 [06:03<44:56,  2.23it/s, Average Loss=0.42, Epoch=2] \u001b[A\n",
            "Train:  11%|█▏        | 770/6760 [06:07<44:15,  2.26it/s, Average Loss=0.42, Epoch=2]\u001b[A\n",
            "Train:  11%|█▏        | 770/6760 [06:07<44:15,  2.26it/s, Average Loss=0.468, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 780/6760 [06:11<43:39,  2.28it/s, Average Loss=0.468, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 780/6760 [06:11<43:39,  2.28it/s, Average Loss=0.412, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 790/6760 [06:15<43:06,  2.31it/s, Average Loss=0.412, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 790/6760 [06:15<43:06,  2.31it/s, Average Loss=0.511, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 800/6760 [06:20<42:39,  2.33it/s, Average Loss=0.511, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 800/6760 [06:20<42:39,  2.33it/s, Average Loss=0.508, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 810/6760 [06:24<42:29,  2.33it/s, Average Loss=0.508, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 810/6760 [06:24<42:29,  2.33it/s, Average Loss=0.484, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 820/6760 [06:28<42:17,  2.34it/s, Average Loss=0.484, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 820/6760 [06:28<42:17,  2.34it/s, Average Loss=0.386, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 830/6760 [06:32<41:59,  2.35it/s, Average Loss=0.386, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 830/6760 [06:32<41:59,  2.35it/s, Average Loss=0.52, Epoch=2] \u001b[A\n",
            "Train:  12%|█▏        | 840/6760 [06:36<41:45,  2.36it/s, Average Loss=0.52, Epoch=2]\u001b[A\n",
            "Train:  12%|█▏        | 840/6760 [06:36<41:45,  2.36it/s, Average Loss=0.442, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 850/6760 [06:41<41:42,  2.36it/s, Average Loss=0.442, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 850/6760 [06:41<41:42,  2.36it/s, Average Loss=0.416, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 860/6760 [06:45<41:39,  2.36it/s, Average Loss=0.416, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 860/6760 [06:45<41:39,  2.36it/s, Average Loss=0.443, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 870/6760 [06:49<41:27,  2.37it/s, Average Loss=0.443, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 870/6760 [06:49<41:27,  2.37it/s, Average Loss=0.406, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 880/6760 [06:53<41:19,  2.37it/s, Average Loss=0.406, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 880/6760 [06:53<41:19,  2.37it/s, Average Loss=0.508, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 890/6760 [06:58<41:18,  2.37it/s, Average Loss=0.508, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 890/6760 [06:58<41:18,  2.37it/s, Average Loss=0.503, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 900/6760 [07:02<41:19,  2.36it/s, Average Loss=0.503, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 900/6760 [07:02<41:19,  2.36it/s, Average Loss=0.487, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 910/6760 [07:06<41:11,  2.37it/s, Average Loss=0.487, Epoch=2]\u001b[A\n",
            "Train:  13%|█▎        | 910/6760 [07:06<41:11,  2.37it/s, Average Loss=0.433, Epoch=2]\u001b[A\n",
            "Train:  14%|█▎        | 920/6760 [07:10<41:02,  2.37it/s, Average Loss=0.433, Epoch=2]\u001b[A\n",
            "Train:  14%|█▎        | 920/6760 [07:10<41:02,  2.37it/s, Average Loss=0.346, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 930/6760 [07:14<41:02,  2.37it/s, Average Loss=0.346, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 930/6760 [07:14<41:02,  2.37it/s, Average Loss=0.454, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 940/6760 [07:19<41:06,  2.36it/s, Average Loss=0.454, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 940/6760 [07:19<41:06,  2.36it/s, Average Loss=0.474, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 950/6760 [07:23<41:01,  2.36it/s, Average Loss=0.474, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 950/6760 [07:23<41:01,  2.36it/s, Average Loss=0.405, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 960/6760 [07:27<40:54,  2.36it/s, Average Loss=0.405, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 960/6760 [07:27<40:54,  2.36it/s, Average Loss=0.349, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 970/6760 [07:31<40:51,  2.36it/s, Average Loss=0.349, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 970/6760 [07:31<40:51,  2.36it/s, Average Loss=0.354, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 980/6760 [07:36<40:51,  2.36it/s, Average Loss=0.354, Epoch=2]\u001b[A\n",
            "Train:  14%|█▍        | 980/6760 [07:36<40:51,  2.36it/s, Average Loss=0.347, Epoch=2]\u001b[A\n",
            "Train:  15%|█▍        | 990/6760 [07:40<40:45,  2.36it/s, Average Loss=0.347, Epoch=2]\u001b[A\n",
            "Train:  15%|█▍        | 990/6760 [07:40<40:45,  2.36it/s, Average Loss=0.482, Epoch=2]\u001b[A\n",
            "Train:  15%|█▍        | 1000/6760 [07:44<40:36,  2.36it/s, Average Loss=0.482, Epoch=2]\u001b[A\n",
            "Train:  15%|█▍        | 1000/6760 [07:44<40:36,  2.36it/s, Average Loss=0.471, Epoch=2]\u001b[A\n",
            "Train:  15%|█▍        | 1010/6760 [07:48<40:31,  2.36it/s, Average Loss=0.471, Epoch=2]\u001b[A\n",
            "Train:  15%|█▍        | 1010/6760 [07:48<40:31,  2.36it/s, Average Loss=0.336, Epoch=2]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 2, Macro F1: 0.5207154150518158, Validation Loss: 0.7282191965469095, Time per Epoch: 152.42829608917236]\n",
            "Begin Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  15%|█▌        | 1020/6760 [08:09<1:26:39,  1.10it/s, Average Loss=0.336, Epoch=2]\u001b[A\n",
            "Train:  15%|█▌        | 1020/6760 [08:09<1:26:39,  1.10it/s, Average Loss=0.48, Epoch=3] \u001b[A\n",
            "Train:  15%|█▌        | 1030/6760 [08:13<1:12:37,  1.32it/s, Average Loss=0.48, Epoch=3]\u001b[A\n",
            "Train:  15%|█▌        | 1030/6760 [08:13<1:12:37,  1.32it/s, Average Loss=0.413, Epoch=3]\u001b[A\n",
            "Train:  15%|█▌        | 1040/6760 [08:17<1:02:42,  1.52it/s, Average Loss=0.413, Epoch=3]\u001b[A\n",
            "Train:  15%|█▌        | 1040/6760 [08:17<1:02:42,  1.52it/s, Average Loss=0.384, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1050/6760 [08:21<55:49,  1.70it/s, Average Loss=0.384, Epoch=3]  \u001b[A\n",
            "Train:  16%|█▌        | 1050/6760 [08:21<55:49,  1.70it/s, Average Loss=0.294, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1060/6760 [08:26<51:12,  1.86it/s, Average Loss=0.294, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1060/6760 [08:26<51:12,  1.86it/s, Average Loss=0.387, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1070/6760 [08:30<47:58,  1.98it/s, Average Loss=0.387, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1070/6760 [08:30<47:58,  1.98it/s, Average Loss=0.331, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1080/6760 [08:34<45:38,  2.07it/s, Average Loss=0.331, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1080/6760 [08:34<45:38,  2.07it/s, Average Loss=0.357, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1090/6760 [08:38<43:58,  2.15it/s, Average Loss=0.357, Epoch=3]\u001b[A\n",
            "Train:  16%|█▌        | 1090/6760 [08:38<43:58,  2.15it/s, Average Loss=0.422, Epoch=3]\u001b[A\n",
            "Train:  16%|█▋        | 1100/6760 [08:43<42:51,  2.20it/s, Average Loss=0.422, Epoch=3]\u001b[A\n",
            "Train:  16%|█▋        | 1100/6760 [08:43<42:51,  2.20it/s, Average Loss=0.278, Epoch=3]\u001b[A\n",
            "Train:  16%|█▋        | 1110/6760 [08:47<42:04,  2.24it/s, Average Loss=0.278, Epoch=3]\u001b[A\n",
            "Train:  16%|█▋        | 1110/6760 [08:47<42:04,  2.24it/s, Average Loss=0.363, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1120/6760 [08:51<41:18,  2.28it/s, Average Loss=0.363, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1120/6760 [08:51<41:18,  2.28it/s, Average Loss=0.305, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1130/6760 [08:55<40:46,  2.30it/s, Average Loss=0.305, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1130/6760 [08:55<40:46,  2.30it/s, Average Loss=0.302, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1140/6760 [09:00<40:24,  2.32it/s, Average Loss=0.302, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1140/6760 [09:00<40:24,  2.32it/s, Average Loss=0.331, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1150/6760 [09:04<40:10,  2.33it/s, Average Loss=0.331, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1150/6760 [09:04<40:10,  2.33it/s, Average Loss=0.37, Epoch=3] \u001b[A\n",
            "Train:  17%|█▋        | 1160/6760 [09:08<39:48,  2.34it/s, Average Loss=0.37, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1160/6760 [09:08<39:48,  2.34it/s, Average Loss=0.229, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1170/6760 [09:12<39:33,  2.36it/s, Average Loss=0.229, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1170/6760 [09:12<39:33,  2.36it/s, Average Loss=0.415, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1180/6760 [09:17<39:25,  2.36it/s, Average Loss=0.415, Epoch=3]\u001b[A\n",
            "Train:  17%|█▋        | 1180/6760 [09:17<39:25,  2.36it/s, Average Loss=0.244, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1190/6760 [09:21<39:23,  2.36it/s, Average Loss=0.244, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1190/6760 [09:21<39:23,  2.36it/s, Average Loss=0.3, Epoch=3]  \u001b[A\n",
            "Train:  18%|█▊        | 1200/6760 [09:25<39:09,  2.37it/s, Average Loss=0.3, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1200/6760 [09:25<39:09,  2.37it/s, Average Loss=0.309, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1210/6760 [09:29<38:59,  2.37it/s, Average Loss=0.309, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1210/6760 [09:29<38:59,  2.37it/s, Average Loss=0.343, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1220/6760 [09:33<38:56,  2.37it/s, Average Loss=0.343, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1220/6760 [09:33<38:56,  2.37it/s, Average Loss=0.37, Epoch=3] \u001b[A\n",
            "Train:  18%|█▊        | 1230/6760 [09:38<38:56,  2.37it/s, Average Loss=0.37, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1230/6760 [09:38<38:56,  2.37it/s, Average Loss=0.372, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1240/6760 [09:42<38:50,  2.37it/s, Average Loss=0.372, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1240/6760 [09:42<38:50,  2.37it/s, Average Loss=0.35, Epoch=3] \u001b[A\n",
            "Train:  18%|█▊        | 1250/6760 [09:46<38:41,  2.37it/s, Average Loss=0.35, Epoch=3]\u001b[A\n",
            "Train:  18%|█▊        | 1250/6760 [09:46<38:41,  2.37it/s, Average Loss=0.324, Epoch=3]\u001b[A\n",
            "Train:  19%|█▊        | 1260/6760 [09:50<38:38,  2.37it/s, Average Loss=0.324, Epoch=3]\u001b[A\n",
            "Train:  19%|█▊        | 1260/6760 [09:50<38:38,  2.37it/s, Average Loss=0.279, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1270/6760 [09:55<38:41,  2.36it/s, Average Loss=0.279, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1270/6760 [09:55<38:41,  2.36it/s, Average Loss=0.337, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1280/6760 [09:59<38:39,  2.36it/s, Average Loss=0.337, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1280/6760 [09:59<38:39,  2.36it/s, Average Loss=0.396, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1290/6760 [10:03<38:32,  2.37it/s, Average Loss=0.396, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1290/6760 [10:03<38:32,  2.37it/s, Average Loss=0.296, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1300/6760 [10:07<38:28,  2.37it/s, Average Loss=0.296, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1300/6760 [10:07<38:28,  2.37it/s, Average Loss=0.27, Epoch=3] \u001b[A\n",
            "Train:  19%|█▉        | 1310/6760 [10:11<38:29,  2.36it/s, Average Loss=0.27, Epoch=3]\u001b[A\n",
            "Train:  19%|█▉        | 1310/6760 [10:11<38:29,  2.36it/s, Average Loss=0.264, Epoch=3]\u001b[A\n",
            "Train:  20%|█▉        | 1320/6760 [10:16<38:25,  2.36it/s, Average Loss=0.264, Epoch=3]\u001b[A\n",
            "Train:  20%|█▉        | 1320/6760 [10:16<38:25,  2.36it/s, Average Loss=0.36, Epoch=3] \u001b[A\n",
            "Train:  20%|█▉        | 1330/6760 [10:20<38:18,  2.36it/s, Average Loss=0.36, Epoch=3]\u001b[A\n",
            "Train:  20%|█▉        | 1330/6760 [10:20<38:18,  2.36it/s, Average Loss=0.39, Epoch=3]\u001b[A\n",
            "Train:  20%|█▉        | 1340/6760 [10:24<38:11,  2.36it/s, Average Loss=0.39, Epoch=3]\u001b[A\n",
            "Train:  20%|█▉        | 1340/6760 [10:24<38:11,  2.36it/s, Average Loss=0.275, Epoch=3]\u001b[A\n",
            "Train:  20%|█▉        | 1350/6760 [10:28<38:15,  2.36it/s, Average Loss=0.275, Epoch=3]\u001b[A\n",
            "Train:  20%|█▉        | 1350/6760 [10:28<38:15,  2.36it/s, Average Loss=0.214, Epoch=3]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 3, Macro F1: 0.5444550727893933, Validation Loss: 0.8983606952567433, Time per Epoch: 152.41724371910095]\n",
            "Begin Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  20%|██        | 1360/6760 [10:47<1:17:04,  1.17it/s, Average Loss=0.214, Epoch=3]\u001b[A\n",
            "Train:  20%|██        | 1360/6760 [10:47<1:17:04,  1.17it/s, Average Loss=0.441, Epoch=4]\u001b[A\n",
            "Train:  20%|██        | 1370/6760 [10:51<1:05:08,  1.38it/s, Average Loss=0.441, Epoch=4]\u001b[A\n",
            "Train:  20%|██        | 1370/6760 [10:51<1:05:08,  1.38it/s, Average Loss=0.201, Epoch=4]\u001b[A\n",
            "Train:  20%|██        | 1380/6760 [10:55<56:47,  1.58it/s, Average Loss=0.201, Epoch=4]  \u001b[A\n",
            "Train:  20%|██        | 1380/6760 [10:55<56:47,  1.58it/s, Average Loss=0.232, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1390/6760 [11:00<51:02,  1.75it/s, Average Loss=0.232, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1390/6760 [11:00<51:02,  1.75it/s, Average Loss=0.217, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1400/6760 [11:04<47:09,  1.89it/s, Average Loss=0.217, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1400/6760 [11:04<47:09,  1.89it/s, Average Loss=0.249, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1410/6760 [11:08<44:21,  2.01it/s, Average Loss=0.249, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1410/6760 [11:08<44:21,  2.01it/s, Average Loss=0.152, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1420/6760 [11:12<42:19,  2.10it/s, Average Loss=0.152, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1420/6760 [11:12<42:19,  2.10it/s, Average Loss=0.244, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1430/6760 [11:17<40:54,  2.17it/s, Average Loss=0.244, Epoch=4]\u001b[A\n",
            "Train:  21%|██        | 1430/6760 [11:17<40:54,  2.17it/s, Average Loss=0.299, Epoch=4]\u001b[A\n",
            "Train:  21%|██▏       | 1440/6760 [11:21<39:56,  2.22it/s, Average Loss=0.299, Epoch=4]\u001b[A\n",
            "Train:  21%|██▏       | 1440/6760 [11:21<39:56,  2.22it/s, Average Loss=0.24, Epoch=4] \u001b[A\n",
            "Train:  21%|██▏       | 1450/6760 [11:25<39:12,  2.26it/s, Average Loss=0.24, Epoch=4]\u001b[A\n",
            "Train:  21%|██▏       | 1450/6760 [11:25<39:12,  2.26it/s, Average Loss=0.224, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1460/6760 [11:29<38:34,  2.29it/s, Average Loss=0.224, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1460/6760 [11:29<38:34,  2.29it/s, Average Loss=0.22, Epoch=4] \u001b[A\n",
            "Train:  22%|██▏       | 1470/6760 [11:34<38:07,  2.31it/s, Average Loss=0.22, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1470/6760 [11:34<38:07,  2.31it/s, Average Loss=0.219, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1480/6760 [11:38<37:57,  2.32it/s, Average Loss=0.219, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1480/6760 [11:38<37:57,  2.32it/s, Average Loss=0.203, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1490/6760 [11:42<37:41,  2.33it/s, Average Loss=0.203, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1490/6760 [11:42<37:41,  2.33it/s, Average Loss=0.286, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1500/6760 [11:46<37:21,  2.35it/s, Average Loss=0.286, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1500/6760 [11:46<37:21,  2.35it/s, Average Loss=0.162, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1510/6760 [11:51<37:10,  2.35it/s, Average Loss=0.162, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1510/6760 [11:51<37:10,  2.35it/s, Average Loss=0.232, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1520/6760 [11:55<37:05,  2.35it/s, Average Loss=0.232, Epoch=4]\u001b[A\n",
            "Train:  22%|██▏       | 1520/6760 [11:55<37:05,  2.35it/s, Average Loss=0.154, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1530/6760 [11:59<36:59,  2.36it/s, Average Loss=0.154, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1530/6760 [11:59<36:59,  2.36it/s, Average Loss=0.279, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1540/6760 [12:03<36:46,  2.37it/s, Average Loss=0.279, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1540/6760 [12:03<36:46,  2.37it/s, Average Loss=0.204, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1550/6760 [12:08<36:38,  2.37it/s, Average Loss=0.204, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1550/6760 [12:08<36:38,  2.37it/s, Average Loss=0.314, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1560/6760 [12:12<36:36,  2.37it/s, Average Loss=0.314, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1560/6760 [12:12<36:36,  2.37it/s, Average Loss=0.277, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1570/6760 [12:16<36:34,  2.36it/s, Average Loss=0.277, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1570/6760 [12:16<36:34,  2.36it/s, Average Loss=0.237, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1580/6760 [12:20<36:26,  2.37it/s, Average Loss=0.237, Epoch=4]\u001b[A\n",
            "Train:  23%|██▎       | 1580/6760 [12:20<36:26,  2.37it/s, Average Loss=0.243, Epoch=4]\u001b[A\n",
            "Train:  24%|██▎       | 1590/6760 [12:24<36:19,  2.37it/s, Average Loss=0.243, Epoch=4]\u001b[A\n",
            "Train:  24%|██▎       | 1590/6760 [12:24<36:19,  2.37it/s, Average Loss=0.186, Epoch=4]\u001b[A\n",
            "Train:  24%|██▎       | 1600/6760 [12:29<36:21,  2.37it/s, Average Loss=0.186, Epoch=4]\u001b[A\n",
            "Train:  24%|██▎       | 1600/6760 [12:29<36:21,  2.37it/s, Average Loss=0.191, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1610/6760 [12:33<36:21,  2.36it/s, Average Loss=0.191, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1610/6760 [12:33<36:21,  2.36it/s, Average Loss=0.171, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1620/6760 [12:37<36:13,  2.37it/s, Average Loss=0.171, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1620/6760 [12:37<36:13,  2.37it/s, Average Loss=0.295, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1630/6760 [12:41<36:06,  2.37it/s, Average Loss=0.295, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1630/6760 [12:41<36:06,  2.37it/s, Average Loss=0.149, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1640/6760 [12:46<36:05,  2.36it/s, Average Loss=0.149, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1640/6760 [12:46<36:05,  2.36it/s, Average Loss=0.17, Epoch=4] \u001b[A\n",
            "Train:  24%|██▍       | 1650/6760 [12:50<36:08,  2.36it/s, Average Loss=0.17, Epoch=4]\u001b[A\n",
            "Train:  24%|██▍       | 1650/6760 [12:50<36:08,  2.36it/s, Average Loss=0.149, Epoch=4]\u001b[A\n",
            "Train:  25%|██▍       | 1660/6760 [12:54<36:00,  2.36it/s, Average Loss=0.149, Epoch=4]\u001b[A\n",
            "Train:  25%|██▍       | 1660/6760 [12:54<36:00,  2.36it/s, Average Loss=0.195, Epoch=4]\u001b[A\n",
            "Train:  25%|██▍       | 1670/6760 [12:58<35:54,  2.36it/s, Average Loss=0.195, Epoch=4]\u001b[A\n",
            "Train:  25%|██▍       | 1670/6760 [12:58<35:54,  2.36it/s, Average Loss=0.218, Epoch=4]\u001b[A\n",
            "Train:  25%|██▍       | 1680/6760 [13:03<35:52,  2.36it/s, Average Loss=0.218, Epoch=4]\u001b[A\n",
            "Train:  25%|██▍       | 1680/6760 [13:03<35:52,  2.36it/s, Average Loss=0.153, Epoch=4]\u001b[A\n",
            "Train:  25%|██▌       | 1690/6760 [13:07<35:47,  2.36it/s, Average Loss=0.153, Epoch=4]\u001b[A\n",
            "Train:  25%|██▌       | 1690/6760 [13:07<35:47,  2.36it/s, Average Loss=0.133, Epoch=4]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 4, Macro F1: 0.5038496688305829, Validation Loss: 1.044020069199939, Time per Epoch: 152.457102060318]\n",
            "Begin Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  25%|██▌       | 1700/6760 [13:20<59:17,  1.42it/s, Average Loss=0.133, Epoch=4]\u001b[A\n",
            "Train:  25%|██▌       | 1700/6760 [13:20<59:17,  1.42it/s, Average Loss=0.255, Epoch=5]\u001b[A\n",
            "Train:  25%|██▌       | 1710/6760 [13:25<52:07,  1.61it/s, Average Loss=0.255, Epoch=5]\u001b[A\n",
            "Train:  25%|██▌       | 1710/6760 [13:25<52:07,  1.61it/s, Average Loss=0.178, Epoch=5]\u001b[A\n",
            "Train:  25%|██▌       | 1720/6760 [13:29<46:59,  1.79it/s, Average Loss=0.178, Epoch=5]\u001b[A\n",
            "Train:  25%|██▌       | 1720/6760 [13:29<46:59,  1.79it/s, Average Loss=0.149, Epoch=5]\u001b[A\n",
            "Train:  26%|██▌       | 1730/6760 [13:33<43:24,  1.93it/s, Average Loss=0.149, Epoch=5]\u001b[A\n",
            "Train:  26%|██▌       | 1730/6760 [13:33<43:24,  1.93it/s, Average Loss=0.18, Epoch=5] \u001b[A\n",
            "Train:  26%|██▌       | 1740/6760 [13:37<40:56,  2.04it/s, Average Loss=0.18, Epoch=5]\u001b[A\n",
            "Train:  26%|██▌       | 1740/6760 [13:37<40:56,  2.04it/s, Average Loss=0.18, Epoch=5]\u001b[A\n",
            "Train:  26%|██▌       | 1750/6760 [13:41<39:11,  2.13it/s, Average Loss=0.18, Epoch=5]\u001b[A\n",
            "Train:  26%|██▌       | 1750/6760 [13:41<39:11,  2.13it/s, Average Loss=0.1, Epoch=5] \u001b[A\n",
            "Train:  26%|██▌       | 1760/6760 [13:46<37:54,  2.20it/s, Average Loss=0.1, Epoch=5]\u001b[A\n",
            "Train:  26%|██▌       | 1760/6760 [13:46<37:54,  2.20it/s, Average Loss=0.176, Epoch=5]\u001b[A\n",
            "Train:  26%|██▌       | 1770/6760 [13:50<36:57,  2.25it/s, Average Loss=0.176, Epoch=5]\u001b[A\n",
            "Train:  26%|██▌       | 1770/6760 [13:50<36:57,  2.25it/s, Average Loss=0.18, Epoch=5] \u001b[A\n",
            "Train:  26%|██▋       | 1780/6760 [13:54<36:17,  2.29it/s, Average Loss=0.18, Epoch=5]\u001b[A\n",
            "Train:  26%|██▋       | 1780/6760 [13:54<36:17,  2.29it/s, Average Loss=0.227, Epoch=5]\u001b[A\n",
            "Train:  26%|██▋       | 1790/6760 [13:58<35:56,  2.30it/s, Average Loss=0.227, Epoch=5]\u001b[A\n",
            "Train:  26%|██▋       | 1790/6760 [13:58<35:56,  2.30it/s, Average Loss=0.157, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1800/6760 [14:03<35:33,  2.32it/s, Average Loss=0.157, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1800/6760 [14:03<35:33,  2.32it/s, Average Loss=0.168, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1810/6760 [14:07<35:14,  2.34it/s, Average Loss=0.168, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1810/6760 [14:07<35:14,  2.34it/s, Average Loss=0.252, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1820/6760 [14:11<35:02,  2.35it/s, Average Loss=0.252, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1820/6760 [14:11<35:02,  2.35it/s, Average Loss=0.161, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1830/6760 [14:15<34:56,  2.35it/s, Average Loss=0.161, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1830/6760 [14:15<34:56,  2.35it/s, Average Loss=0.196, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1840/6760 [14:19<34:48,  2.36it/s, Average Loss=0.196, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1840/6760 [14:19<34:48,  2.36it/s, Average Loss=0.113, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1850/6760 [14:24<34:39,  2.36it/s, Average Loss=0.113, Epoch=5]\u001b[A\n",
            "Train:  27%|██▋       | 1850/6760 [14:24<34:39,  2.36it/s, Average Loss=0.174, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1860/6760 [14:28<34:32,  2.36it/s, Average Loss=0.174, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1860/6760 [14:28<34:32,  2.36it/s, Average Loss=0.107, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1870/6760 [14:32<34:33,  2.36it/s, Average Loss=0.107, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1870/6760 [14:32<34:33,  2.36it/s, Average Loss=0.187, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1880/6760 [14:36<34:28,  2.36it/s, Average Loss=0.187, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1880/6760 [14:36<34:28,  2.36it/s, Average Loss=0.153, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1890/6760 [14:41<34:20,  2.36it/s, Average Loss=0.153, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1890/6760 [14:41<34:20,  2.36it/s, Average Loss=0.198, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1900/6760 [14:45<34:14,  2.37it/s, Average Loss=0.198, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1900/6760 [14:45<34:14,  2.37it/s, Average Loss=0.24, Epoch=5] \u001b[A\n",
            "Train:  28%|██▊       | 1910/6760 [14:49<34:14,  2.36it/s, Average Loss=0.24, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1910/6760 [14:49<34:14,  2.36it/s, Average Loss=0.0782, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1920/6760 [14:53<34:11,  2.36it/s, Average Loss=0.0782, Epoch=5]\u001b[A\n",
            "Train:  28%|██▊       | 1920/6760 [14:53<34:11,  2.36it/s, Average Loss=0.188, Epoch=5] \u001b[A\n",
            "Train:  29%|██▊       | 1930/6760 [14:57<34:03,  2.36it/s, Average Loss=0.188, Epoch=5]\u001b[A\n",
            "Train:  29%|██▊       | 1930/6760 [14:57<34:03,  2.36it/s, Average Loss=0.121, Epoch=5]\u001b[A\n",
            "Train:  29%|██▊       | 1940/6760 [15:02<33:55,  2.37it/s, Average Loss=0.121, Epoch=5]\u001b[A\n",
            "Train:  29%|██▊       | 1940/6760 [15:02<33:55,  2.37it/s, Average Loss=0.122, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1950/6760 [15:06<33:55,  2.36it/s, Average Loss=0.122, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1950/6760 [15:06<33:55,  2.36it/s, Average Loss=0.107, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1960/6760 [15:10<33:53,  2.36it/s, Average Loss=0.107, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1960/6760 [15:10<33:53,  2.36it/s, Average Loss=0.146, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1970/6760 [15:14<33:46,  2.36it/s, Average Loss=0.146, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1970/6760 [15:14<33:46,  2.36it/s, Average Loss=0.0555, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1980/6760 [15:19<33:38,  2.37it/s, Average Loss=0.0555, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1980/6760 [15:19<33:38,  2.37it/s, Average Loss=0.111, Epoch=5] \u001b[A\n",
            "Train:  29%|██▉       | 1990/6760 [15:23<33:37,  2.36it/s, Average Loss=0.111, Epoch=5]\u001b[A\n",
            "Train:  29%|██▉       | 1990/6760 [15:23<33:37,  2.36it/s, Average Loss=0.168, Epoch=5]\u001b[A\n",
            "Train:  30%|██▉       | 2000/6760 [15:27<33:38,  2.36it/s, Average Loss=0.168, Epoch=5]\u001b[A\n",
            "Train:  30%|██▉       | 2000/6760 [15:27<33:38,  2.36it/s, Average Loss=0.164, Epoch=5]\u001b[A\n",
            "Train:  30%|██▉       | 2010/6760 [15:31<33:29,  2.36it/s, Average Loss=0.164, Epoch=5]\u001b[A\n",
            "Train:  30%|██▉       | 2010/6760 [15:31<33:29,  2.36it/s, Average Loss=0.217, Epoch=5]\u001b[A\n",
            "Train:  30%|██▉       | 2020/6760 [15:36<33:22,  2.37it/s, Average Loss=0.217, Epoch=5]\u001b[A\n",
            "Train:  30%|██▉       | 2020/6760 [15:36<33:22,  2.37it/s, Average Loss=0.0931, Epoch=5]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 5, Macro F1: 0.5188610889617992, Validation Loss: 1.1200813581777174, Time per Epoch: 152.10613131523132]\n",
            "Begin Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  30%|███       | 2030/6760 [15:49<55:15,  1.43it/s, Average Loss=0.0931, Epoch=5]\u001b[A\n",
            "Train:  30%|███       | 2030/6760 [15:49<55:15,  1.43it/s, Average Loss=0.108, Epoch=6] \u001b[A\n",
            "Train:  30%|███       | 2040/6760 [15:53<48:31,  1.62it/s, Average Loss=0.108, Epoch=6]\u001b[A\n",
            "Train:  30%|███       | 2040/6760 [15:53<48:31,  1.62it/s, Average Loss=0.214, Epoch=6]\u001b[A\n",
            "Train:  30%|███       | 2050/6760 [15:57<43:53,  1.79it/s, Average Loss=0.214, Epoch=6]\u001b[A\n",
            "Train:  30%|███       | 2050/6760 [15:58<43:53,  1.79it/s, Average Loss=0.108, Epoch=6]\u001b[A\n",
            "Train:  30%|███       | 2060/6760 [16:02<40:40,  1.93it/s, Average Loss=0.108, Epoch=6]\u001b[A\n",
            "Train:  30%|███       | 2060/6760 [16:02<40:40,  1.93it/s, Average Loss=0.0795, Epoch=6]\u001b[A\n",
            "Train:  31%|███       | 2070/6760 [16:06<38:17,  2.04it/s, Average Loss=0.0795, Epoch=6]\u001b[A\n",
            "Train:  31%|███       | 2070/6760 [16:06<38:17,  2.04it/s, Average Loss=0.111, Epoch=6] \u001b[A\n",
            "Train:  31%|███       | 2080/6760 [16:10<36:38,  2.13it/s, Average Loss=0.111, Epoch=6]\u001b[A\n",
            "Train:  31%|███       | 2080/6760 [16:10<36:38,  2.13it/s, Average Loss=0.115, Epoch=6]\u001b[A\n",
            "Train:  31%|███       | 2090/6760 [16:14<35:29,  2.19it/s, Average Loss=0.115, Epoch=6]\u001b[A\n",
            "Train:  31%|███       | 2090/6760 [16:14<35:29,  2.19it/s, Average Loss=0.1, Epoch=6]  \u001b[A\n",
            "Train:  31%|███       | 2100/6760 [16:19<34:44,  2.24it/s, Average Loss=0.1, Epoch=6]\u001b[A\n",
            "Train:  31%|███       | 2100/6760 [16:19<34:44,  2.24it/s, Average Loss=0.206, Epoch=6]\u001b[A\n",
            "Train:  31%|███       | 2110/6760 [16:23<34:03,  2.28it/s, Average Loss=0.206, Epoch=6]\u001b[A\n",
            "Train:  31%|███       | 2110/6760 [16:23<34:03,  2.28it/s, Average Loss=0.131, Epoch=6]\u001b[A\n",
            "Train:  31%|███▏      | 2120/6760 [16:27<33:35,  2.30it/s, Average Loss=0.131, Epoch=6]\u001b[A\n",
            "Train:  31%|███▏      | 2120/6760 [16:27<33:35,  2.30it/s, Average Loss=0.176, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2130/6760 [16:31<33:16,  2.32it/s, Average Loss=0.176, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2130/6760 [16:31<33:16,  2.32it/s, Average Loss=0.112, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2140/6760 [16:36<33:05,  2.33it/s, Average Loss=0.112, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2140/6760 [16:36<33:05,  2.33it/s, Average Loss=0.105, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2150/6760 [16:40<32:50,  2.34it/s, Average Loss=0.105, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2150/6760 [16:40<32:50,  2.34it/s, Average Loss=0.136, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2160/6760 [16:44<32:37,  2.35it/s, Average Loss=0.136, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2160/6760 [16:44<32:37,  2.35it/s, Average Loss=0.123, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2170/6760 [16:48<32:29,  2.35it/s, Average Loss=0.123, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2170/6760 [16:48<32:29,  2.35it/s, Average Loss=0.141, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2180/6760 [16:53<32:27,  2.35it/s, Average Loss=0.141, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2180/6760 [16:53<32:27,  2.35it/s, Average Loss=0.146, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2190/6760 [16:57<32:19,  2.36it/s, Average Loss=0.146, Epoch=6]\u001b[A\n",
            "Train:  32%|███▏      | 2190/6760 [16:57<32:19,  2.36it/s, Average Loss=0.0925, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2200/6760 [17:01<32:09,  2.36it/s, Average Loss=0.0925, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2200/6760 [17:01<32:09,  2.36it/s, Average Loss=0.0975, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2210/6760 [17:05<32:03,  2.37it/s, Average Loss=0.0975, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2210/6760 [17:05<32:03,  2.37it/s, Average Loss=0.116, Epoch=6] \u001b[A\n",
            "Train:  33%|███▎      | 2220/6760 [17:09<32:01,  2.36it/s, Average Loss=0.116, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2220/6760 [17:09<32:01,  2.36it/s, Average Loss=0.0916, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2230/6760 [17:14<31:57,  2.36it/s, Average Loss=0.0916, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2230/6760 [17:14<31:57,  2.36it/s, Average Loss=0.134, Epoch=6] \u001b[A\n",
            "Train:  33%|███▎      | 2240/6760 [17:18<31:47,  2.37it/s, Average Loss=0.134, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2240/6760 [17:18<31:47,  2.37it/s, Average Loss=0.132, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2250/6760 [17:22<31:42,  2.37it/s, Average Loss=0.132, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2250/6760 [17:22<31:42,  2.37it/s, Average Loss=0.117, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2260/6760 [17:26<31:42,  2.37it/s, Average Loss=0.117, Epoch=6]\u001b[A\n",
            "Train:  33%|███▎      | 2260/6760 [17:26<31:42,  2.37it/s, Average Loss=0.158, Epoch=6]\u001b[A\n",
            "Train:  34%|███▎      | 2270/6760 [17:31<31:39,  2.36it/s, Average Loss=0.158, Epoch=6]\u001b[A\n",
            "Train:  34%|███▎      | 2270/6760 [17:31<31:39,  2.36it/s, Average Loss=0.0739, Epoch=6]\u001b[A\n",
            "Train:  34%|███▎      | 2280/6760 [17:35<31:30,  2.37it/s, Average Loss=0.0739, Epoch=6]\u001b[A\n",
            "Train:  34%|███▎      | 2280/6760 [17:35<31:30,  2.37it/s, Average Loss=0.0963, Epoch=6]\u001b[A\n",
            "Train:  34%|███▍      | 2290/6760 [17:39<31:24,  2.37it/s, Average Loss=0.0963, Epoch=6]\u001b[A\n",
            "Train:  34%|███▍      | 2290/6760 [17:39<31:24,  2.37it/s, Average Loss=0.0845, Epoch=6]\u001b[A\n",
            "Train:  34%|███▍      | 2300/6760 [17:43<31:26,  2.36it/s, Average Loss=0.0845, Epoch=6]\u001b[A\n",
            "Train:  34%|███▍      | 2300/6760 [17:43<31:26,  2.36it/s, Average Loss=0.108, Epoch=6] \u001b[A\n",
            "Train:  34%|███▍      | 2310/6760 [17:48<31:24,  2.36it/s, Average Loss=0.108, Epoch=6]\u001b[A\n",
            "Train:  34%|███▍      | 2310/6760 [17:48<31:24,  2.36it/s, Average Loss=0.0653, Epoch=6]\u001b[A\n",
            "Train:  34%|███▍      | 2320/6760 [17:52<31:16,  2.37it/s, Average Loss=0.0653, Epoch=6]\u001b[A\n",
            "Train:  34%|███▍      | 2320/6760 [17:52<31:16,  2.37it/s, Average Loss=0.104, Epoch=6] \u001b[A\n",
            "Train:  34%|███▍      | 2330/6760 [17:56<31:11,  2.37it/s, Average Loss=0.104, Epoch=6]\u001b[A\n",
            "Train:  34%|███▍      | 2330/6760 [17:56<31:11,  2.37it/s, Average Loss=0.128, Epoch=6]\u001b[A\n",
            "Train:  35%|███▍      | 2340/6760 [18:00<31:11,  2.36it/s, Average Loss=0.128, Epoch=6]\u001b[A\n",
            "Train:  35%|███▍      | 2340/6760 [18:00<31:11,  2.36it/s, Average Loss=0.14, Epoch=6] \u001b[A\n",
            "Train:  35%|███▍      | 2350/6760 [18:04<31:08,  2.36it/s, Average Loss=0.14, Epoch=6]\u001b[A\n",
            "Train:  35%|███▍      | 2350/6760 [18:04<31:08,  2.36it/s, Average Loss=0.178, Epoch=6]\u001b[A\n",
            "Train:  35%|███▍      | 2360/6760 [18:09<31:00,  2.37it/s, Average Loss=0.178, Epoch=6]\u001b[A\n",
            "Train:  35%|███▍      | 2360/6760 [18:09<31:00,  2.37it/s, Average Loss=0.0678, Epoch=6]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 6, Macro F1: 0.5366859178207218, Validation Loss: 1.1788012953691704, Time per Epoch: 152.43746185302734]\n",
            "Begin Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  35%|███▌      | 2370/6760 [18:22<51:43,  1.41it/s, Average Loss=0.0678, Epoch=6]\u001b[A\n",
            "Train:  35%|███▌      | 2370/6760 [18:22<51:43,  1.41it/s, Average Loss=0.115, Epoch=7] \u001b[A\n",
            "Train:  35%|███▌      | 2380/6760 [18:27<45:19,  1.61it/s, Average Loss=0.115, Epoch=7]\u001b[A\n",
            "Train:  35%|███▌      | 2380/6760 [18:27<45:19,  1.61it/s, Average Loss=0.16, Epoch=7] \u001b[A\n",
            "Train:  35%|███▌      | 2390/6760 [18:31<40:49,  1.78it/s, Average Loss=0.16, Epoch=7]\u001b[A\n",
            "Train:  35%|███▌      | 2390/6760 [18:31<40:49,  1.78it/s, Average Loss=0.0792, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2400/6760 [18:35<37:46,  1.92it/s, Average Loss=0.0792, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2400/6760 [18:35<37:46,  1.92it/s, Average Loss=0.0367, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2410/6760 [18:39<35:38,  2.03it/s, Average Loss=0.0367, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2410/6760 [18:39<35:38,  2.03it/s, Average Loss=0.108, Epoch=7] \u001b[A\n",
            "Train:  36%|███▌      | 2420/6760 [18:43<34:01,  2.13it/s, Average Loss=0.108, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2420/6760 [18:43<34:01,  2.13it/s, Average Loss=0.12, Epoch=7] \u001b[A\n",
            "Train:  36%|███▌      | 2430/6760 [18:48<32:52,  2.19it/s, Average Loss=0.12, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2430/6760 [18:48<32:52,  2.19it/s, Average Loss=0.0853, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2440/6760 [18:52<32:07,  2.24it/s, Average Loss=0.0853, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2440/6760 [18:52<32:07,  2.24it/s, Average Loss=0.182, Epoch=7] \u001b[A\n",
            "Train:  36%|███▌      | 2450/6760 [18:56<31:36,  2.27it/s, Average Loss=0.182, Epoch=7]\u001b[A\n",
            "Train:  36%|███▌      | 2450/6760 [18:56<31:36,  2.27it/s, Average Loss=0.0707, Epoch=7]\u001b[A\n",
            "Train:  36%|███▋      | 2460/6760 [19:00<31:08,  2.30it/s, Average Loss=0.0707, Epoch=7]\u001b[A\n",
            "Train:  36%|███▋      | 2460/6760 [19:00<31:08,  2.30it/s, Average Loss=0.105, Epoch=7] \u001b[A\n",
            "Train:  37%|███▋      | 2470/6760 [19:05<30:48,  2.32it/s, Average Loss=0.105, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2470/6760 [19:05<30:48,  2.32it/s, Average Loss=0.0511, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2480/6760 [19:09<30:36,  2.33it/s, Average Loss=0.0511, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2480/6760 [19:09<30:36,  2.33it/s, Average Loss=0.0696, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2490/6760 [19:13<30:28,  2.33it/s, Average Loss=0.0696, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2490/6760 [19:13<30:28,  2.33it/s, Average Loss=0.144, Epoch=7] \u001b[A\n",
            "Train:  37%|███▋      | 2500/6760 [19:17<30:17,  2.34it/s, Average Loss=0.144, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2500/6760 [19:17<30:17,  2.34it/s, Average Loss=0.0619, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2510/6760 [19:22<30:06,  2.35it/s, Average Loss=0.0619, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2510/6760 [19:22<30:06,  2.35it/s, Average Loss=0.0905, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2520/6760 [19:26<30:00,  2.35it/s, Average Loss=0.0905, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2520/6760 [19:26<30:00,  2.35it/s, Average Loss=0.129, Epoch=7] \u001b[A\n",
            "Train:  37%|███▋      | 2530/6760 [19:30<29:58,  2.35it/s, Average Loss=0.129, Epoch=7]\u001b[A\n",
            "Train:  37%|███▋      | 2530/6760 [19:30<29:58,  2.35it/s, Average Loss=0.166, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2540/6760 [19:34<29:52,  2.35it/s, Average Loss=0.166, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2540/6760 [19:34<29:52,  2.35it/s, Average Loss=0.0963, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2550/6760 [19:39<29:43,  2.36it/s, Average Loss=0.0963, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2550/6760 [19:39<29:43,  2.36it/s, Average Loss=0.14, Epoch=7]  \u001b[A\n",
            "Train:  38%|███▊      | 2560/6760 [19:43<29:38,  2.36it/s, Average Loss=0.14, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2560/6760 [19:43<29:38,  2.36it/s, Average Loss=0.0729, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2570/6760 [19:47<29:39,  2.35it/s, Average Loss=0.0729, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2570/6760 [19:47<29:39,  2.35it/s, Average Loss=0.088, Epoch=7] \u001b[A\n",
            "Train:  38%|███▊      | 2580/6760 [19:51<29:33,  2.36it/s, Average Loss=0.088, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2580/6760 [19:51<29:33,  2.36it/s, Average Loss=0.0712, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2590/6760 [19:55<29:24,  2.36it/s, Average Loss=0.0712, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2590/6760 [19:55<29:24,  2.36it/s, Average Loss=0.0775, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2600/6760 [20:00<29:19,  2.36it/s, Average Loss=0.0775, Epoch=7]\u001b[A\n",
            "Train:  38%|███▊      | 2600/6760 [20:00<29:19,  2.36it/s, Average Loss=0.143, Epoch=7] \u001b[A\n",
            "Train:  39%|███▊      | 2610/6760 [20:04<29:18,  2.36it/s, Average Loss=0.143, Epoch=7]\u001b[A\n",
            "Train:  39%|███▊      | 2610/6760 [20:04<29:18,  2.36it/s, Average Loss=0.0923, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2620/6760 [20:08<29:14,  2.36it/s, Average Loss=0.0923, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2620/6760 [20:08<29:14,  2.36it/s, Average Loss=0.0755, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2630/6760 [20:12<29:06,  2.36it/s, Average Loss=0.0755, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2630/6760 [20:12<29:06,  2.36it/s, Average Loss=0.0638, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2640/6760 [20:17<29:00,  2.37it/s, Average Loss=0.0638, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2640/6760 [20:17<29:00,  2.37it/s, Average Loss=0.0568, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2650/6760 [20:21<28:59,  2.36it/s, Average Loss=0.0568, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2650/6760 [20:21<28:59,  2.36it/s, Average Loss=0.0821, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2660/6760 [20:25<28:56,  2.36it/s, Average Loss=0.0821, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2660/6760 [20:25<28:56,  2.36it/s, Average Loss=0.0984, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2670/6760 [20:29<28:49,  2.37it/s, Average Loss=0.0984, Epoch=7]\u001b[A\n",
            "Train:  39%|███▉      | 2670/6760 [20:29<28:49,  2.37it/s, Average Loss=0.121, Epoch=7] \u001b[A\n",
            "Train:  40%|███▉      | 2680/6760 [20:33<28:41,  2.37it/s, Average Loss=0.121, Epoch=7]\u001b[A\n",
            "Train:  40%|███▉      | 2680/6760 [20:34<28:41,  2.37it/s, Average Loss=0.116, Epoch=7]\u001b[A\n",
            "Train:  40%|███▉      | 2690/6760 [20:38<28:40,  2.37it/s, Average Loss=0.116, Epoch=7]\u001b[A\n",
            "Train:  40%|███▉      | 2690/6760 [20:38<28:40,  2.37it/s, Average Loss=0.147, Epoch=7]\u001b[A\n",
            "Train:  40%|███▉      | 2700/6760 [20:42<28:38,  2.36it/s, Average Loss=0.147, Epoch=7]\u001b[A\n",
            "Train:  40%|███▉      | 2700/6760 [20:42<28:38,  2.36it/s, Average Loss=0.0528, Epoch=7]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 7, Macro F1: 0.5335311194157127, Validation Loss: 1.1667471777561098, Time per Epoch: 152.4079887866974]\n",
            "Begin Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  40%|████      | 2710/6760 [20:56<47:35,  1.42it/s, Average Loss=0.0528, Epoch=7]\u001b[A\n",
            "Train:  40%|████      | 2710/6760 [20:56<47:35,  1.42it/s, Average Loss=0.0881, Epoch=8]\u001b[A\n",
            "Train:  40%|████      | 2720/6760 [21:00<41:49,  1.61it/s, Average Loss=0.0881, Epoch=8]\u001b[A\n",
            "Train:  40%|████      | 2720/6760 [21:00<41:49,  1.61it/s, Average Loss=0.064, Epoch=8] \u001b[A\n",
            "Train:  40%|████      | 2730/6760 [21:04<37:41,  1.78it/s, Average Loss=0.064, Epoch=8]\u001b[A\n",
            "Train:  40%|████      | 2730/6760 [21:04<37:41,  1.78it/s, Average Loss=0.0271, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2740/6760 [21:08<34:47,  1.93it/s, Average Loss=0.0271, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2740/6760 [21:08<34:47,  1.93it/s, Average Loss=0.0142, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2750/6760 [21:13<32:49,  2.04it/s, Average Loss=0.0142, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2750/6760 [21:13<32:49,  2.04it/s, Average Loss=0.132, Epoch=8] \u001b[A\n",
            "Train:  41%|████      | 2760/6760 [21:17<31:26,  2.12it/s, Average Loss=0.132, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2760/6760 [21:17<31:26,  2.12it/s, Average Loss=0.0767, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2770/6760 [21:21<30:20,  2.19it/s, Average Loss=0.0767, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2770/6760 [21:21<30:20,  2.19it/s, Average Loss=0.0852, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2780/6760 [21:25<29:35,  2.24it/s, Average Loss=0.0852, Epoch=8]\u001b[A\n",
            "Train:  41%|████      | 2780/6760 [21:25<29:35,  2.24it/s, Average Loss=0.128, Epoch=8] \u001b[A\n",
            "Train:  41%|████▏     | 2790/6760 [21:30<29:07,  2.27it/s, Average Loss=0.128, Epoch=8]\u001b[A\n",
            "Train:  41%|████▏     | 2790/6760 [21:30<29:07,  2.27it/s, Average Loss=0.0658, Epoch=8]\u001b[A\n",
            "Train:  41%|████▏     | 2800/6760 [21:34<28:48,  2.29it/s, Average Loss=0.0658, Epoch=8]\u001b[A\n",
            "Train:  41%|████▏     | 2800/6760 [21:34<28:48,  2.29it/s, Average Loss=0.0734, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2810/6760 [21:38<28:26,  2.31it/s, Average Loss=0.0734, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2810/6760 [21:38<28:26,  2.31it/s, Average Loss=0.0631, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2820/6760 [21:42<28:10,  2.33it/s, Average Loss=0.0631, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2820/6760 [21:42<28:10,  2.33it/s, Average Loss=0.0904, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2830/6760 [21:46<28:00,  2.34it/s, Average Loss=0.0904, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2830/6760 [21:46<28:00,  2.34it/s, Average Loss=0.0809, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2840/6760 [21:51<27:57,  2.34it/s, Average Loss=0.0809, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2840/6760 [21:51<27:57,  2.34it/s, Average Loss=0.0731, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2850/6760 [21:55<27:46,  2.35it/s, Average Loss=0.0731, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2850/6760 [21:55<27:46,  2.35it/s, Average Loss=0.0255, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2860/6760 [21:59<27:38,  2.35it/s, Average Loss=0.0255, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2860/6760 [21:59<27:38,  2.35it/s, Average Loss=0.145, Epoch=8] \u001b[A\n",
            "Train:  42%|████▏     | 2870/6760 [22:03<27:32,  2.35it/s, Average Loss=0.145, Epoch=8]\u001b[A\n",
            "Train:  42%|████▏     | 2870/6760 [22:03<27:32,  2.35it/s, Average Loss=0.0553, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2880/6760 [22:08<27:31,  2.35it/s, Average Loss=0.0553, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2880/6760 [22:08<27:31,  2.35it/s, Average Loss=0.0635, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2890/6760 [22:12<27:24,  2.35it/s, Average Loss=0.0635, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2890/6760 [22:12<27:24,  2.35it/s, Average Loss=0.0929, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2900/6760 [22:16<27:16,  2.36it/s, Average Loss=0.0929, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2900/6760 [22:16<27:16,  2.36it/s, Average Loss=0.0445, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2910/6760 [22:20<27:11,  2.36it/s, Average Loss=0.0445, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2910/6760 [22:20<27:11,  2.36it/s, Average Loss=0.0737, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2920/6760 [22:25<27:10,  2.36it/s, Average Loss=0.0737, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2920/6760 [22:25<27:10,  2.36it/s, Average Loss=0.0661, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2930/6760 [22:29<27:04,  2.36it/s, Average Loss=0.0661, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2930/6760 [22:29<27:04,  2.36it/s, Average Loss=0.068, Epoch=8] \u001b[A\n",
            "Train:  43%|████▎     | 2940/6760 [22:33<26:57,  2.36it/s, Average Loss=0.068, Epoch=8]\u001b[A\n",
            "Train:  43%|████▎     | 2940/6760 [22:33<26:57,  2.36it/s, Average Loss=0.0445, Epoch=8]\u001b[A\n",
            "Train:  44%|████▎     | 2950/6760 [22:37<26:51,  2.36it/s, Average Loss=0.0445, Epoch=8]\u001b[A\n",
            "Train:  44%|████▎     | 2950/6760 [22:37<26:51,  2.36it/s, Average Loss=0.121, Epoch=8] \u001b[A\n",
            "Train:  44%|████▍     | 2960/6760 [22:42<26:51,  2.36it/s, Average Loss=0.121, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 2960/6760 [22:42<26:51,  2.36it/s, Average Loss=0.0885, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 2970/6760 [22:46<26:47,  2.36it/s, Average Loss=0.0885, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 2970/6760 [22:46<26:47,  2.36it/s, Average Loss=0.0718, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 2980/6760 [22:50<26:40,  2.36it/s, Average Loss=0.0718, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 2980/6760 [22:50<26:40,  2.36it/s, Average Loss=0.0359, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 2990/6760 [22:54<26:34,  2.36it/s, Average Loss=0.0359, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 2990/6760 [22:54<26:34,  2.36it/s, Average Loss=0.0622, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 3000/6760 [22:59<26:34,  2.36it/s, Average Loss=0.0622, Epoch=8]\u001b[A\n",
            "Train:  44%|████▍     | 3000/6760 [22:59<26:34,  2.36it/s, Average Loss=0.0456, Epoch=8]\u001b[A\n",
            "Train:  45%|████▍     | 3010/6760 [23:03<26:29,  2.36it/s, Average Loss=0.0456, Epoch=8]\u001b[A\n",
            "Train:  45%|████▍     | 3010/6760 [23:03<26:29,  2.36it/s, Average Loss=0.063, Epoch=8] \u001b[A\n",
            "Train:  45%|████▍     | 3020/6760 [23:07<26:23,  2.36it/s, Average Loss=0.063, Epoch=8]\u001b[A\n",
            "Train:  45%|████▍     | 3020/6760 [23:07<26:23,  2.36it/s, Average Loss=0.102, Epoch=8]\u001b[A\n",
            "Train:  45%|████▍     | 3030/6760 [23:11<26:16,  2.37it/s, Average Loss=0.102, Epoch=8]\u001b[A\n",
            "Train:  45%|████▍     | 3030/6760 [23:11<26:16,  2.37it/s, Average Loss=0.0491, Epoch=8]\u001b[A\n",
            "Train:  45%|████▍     | 3040/6760 [23:15<26:15,  2.36it/s, Average Loss=0.0491, Epoch=8]\u001b[A\n",
            "Train:  45%|████▍     | 3040/6760 [23:15<26:15,  2.36it/s, Average Loss=0.01, Epoch=8]  \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 8, Macro F1: 0.5515786426259154, Validation Loss: 1.3141182106594707, Time per Epoch: 152.52359557151794]\n",
            "Begin Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  45%|████▌     | 3050/6760 [23:34<52:20,  1.18it/s, Average Loss=0.01, Epoch=8]\u001b[A\n",
            "Train:  45%|████▌     | 3050/6760 [23:34<52:20,  1.18it/s, Average Loss=0.0868, Epoch=9]\u001b[A\n",
            "Train:  45%|████▌     | 3060/6760 [23:38<44:19,  1.39it/s, Average Loss=0.0868, Epoch=9]\u001b[A\n",
            "Train:  45%|████▌     | 3060/6760 [23:38<44:19,  1.39it/s, Average Loss=0.0727, Epoch=9]\u001b[A\n",
            "Train:  45%|████▌     | 3070/6760 [23:42<38:42,  1.59it/s, Average Loss=0.0727, Epoch=9]\u001b[A\n",
            "Train:  45%|████▌     | 3070/6760 [23:42<38:42,  1.59it/s, Average Loss=0.0257, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3080/6760 [23:46<34:46,  1.76it/s, Average Loss=0.0257, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3080/6760 [23:46<34:46,  1.76it/s, Average Loss=0.0926, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3090/6760 [23:51<32:07,  1.90it/s, Average Loss=0.0926, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3090/6760 [23:51<32:07,  1.90it/s, Average Loss=0.0159, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3100/6760 [23:55<30:13,  2.02it/s, Average Loss=0.0159, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3100/6760 [23:55<30:13,  2.02it/s, Average Loss=0.0161, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3110/6760 [23:59<28:50,  2.11it/s, Average Loss=0.0161, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3110/6760 [23:59<28:50,  2.11it/s, Average Loss=0.0914, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3120/6760 [24:03<27:52,  2.18it/s, Average Loss=0.0914, Epoch=9]\u001b[A\n",
            "Train:  46%|████▌     | 3120/6760 [24:03<27:52,  2.18it/s, Average Loss=0.0619, Epoch=9]\u001b[A\n",
            "Train:  46%|████▋     | 3130/6760 [24:08<27:13,  2.22it/s, Average Loss=0.0619, Epoch=9]\u001b[A\n",
            "Train:  46%|████▋     | 3130/6760 [24:08<27:13,  2.22it/s, Average Loss=0.0767, Epoch=9]\u001b[A\n",
            "Train:  46%|████▋     | 3140/6760 [24:12<26:45,  2.25it/s, Average Loss=0.0767, Epoch=9]\u001b[A\n",
            "Train:  46%|████▋     | 3140/6760 [24:12<26:45,  2.25it/s, Average Loss=0.00911, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3150/6760 [24:16<26:19,  2.29it/s, Average Loss=0.00911, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3150/6760 [24:16<26:19,  2.29it/s, Average Loss=0.0144, Epoch=9] \u001b[A\n",
            "Train:  47%|████▋     | 3160/6760 [24:20<25:59,  2.31it/s, Average Loss=0.0144, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3160/6760 [24:21<25:59,  2.31it/s, Average Loss=0.0935, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3170/6760 [24:25<25:49,  2.32it/s, Average Loss=0.0935, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3170/6760 [24:25<25:49,  2.32it/s, Average Loss=0.0831, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3180/6760 [24:29<25:39,  2.33it/s, Average Loss=0.0831, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3180/6760 [24:29<25:39,  2.33it/s, Average Loss=0.00958, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3190/6760 [24:33<25:24,  2.34it/s, Average Loss=0.00958, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3190/6760 [24:33<25:24,  2.34it/s, Average Loss=0.0686, Epoch=9] \u001b[A\n",
            "Train:  47%|████▋     | 3200/6760 [24:37<25:14,  2.35it/s, Average Loss=0.0686, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3200/6760 [24:37<25:14,  2.35it/s, Average Loss=0.0652, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3210/6760 [24:42<25:07,  2.35it/s, Average Loss=0.0652, Epoch=9]\u001b[A\n",
            "Train:  47%|████▋     | 3210/6760 [24:42<25:07,  2.35it/s, Average Loss=0.0844, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3220/6760 [24:46<25:03,  2.35it/s, Average Loss=0.0844, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3220/6760 [24:46<25:03,  2.35it/s, Average Loss=0.102, Epoch=9] \u001b[A\n",
            "Train:  48%|████▊     | 3230/6760 [24:50<24:52,  2.36it/s, Average Loss=0.102, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3230/6760 [24:50<24:52,  2.36it/s, Average Loss=0.0899, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3240/6760 [24:54<24:44,  2.37it/s, Average Loss=0.0899, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3240/6760 [24:54<24:44,  2.37it/s, Average Loss=0.039, Epoch=9] \u001b[A\n",
            "Train:  48%|████▊     | 3250/6760 [24:59<24:43,  2.37it/s, Average Loss=0.039, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3250/6760 [24:59<24:43,  2.37it/s, Average Loss=0.0261, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3260/6760 [25:03<24:42,  2.36it/s, Average Loss=0.0261, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3260/6760 [25:03<24:42,  2.36it/s, Average Loss=0.0697, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3270/6760 [25:07<24:35,  2.37it/s, Average Loss=0.0697, Epoch=9]\u001b[A\n",
            "Train:  48%|████▊     | 3270/6760 [25:07<24:35,  2.37it/s, Average Loss=0.0653, Epoch=9]\u001b[A\n",
            "Train:  49%|████▊     | 3280/6760 [25:11<24:29,  2.37it/s, Average Loss=0.0653, Epoch=9]\u001b[A\n",
            "Train:  49%|████▊     | 3280/6760 [25:11<24:29,  2.37it/s, Average Loss=0.0398, Epoch=9]\u001b[A\n",
            "Train:  49%|████▊     | 3290/6760 [25:15<24:25,  2.37it/s, Average Loss=0.0398, Epoch=9]\u001b[A\n",
            "Train:  49%|████▊     | 3290/6760 [25:15<24:25,  2.37it/s, Average Loss=0.0446, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3300/6760 [25:20<24:25,  2.36it/s, Average Loss=0.0446, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3300/6760 [25:20<24:25,  2.36it/s, Average Loss=0.045, Epoch=9] \u001b[A\n",
            "Train:  49%|████▉     | 3310/6760 [25:24<24:20,  2.36it/s, Average Loss=0.045, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3310/6760 [25:24<24:20,  2.36it/s, Average Loss=0.0621, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3320/6760 [25:28<24:15,  2.36it/s, Average Loss=0.0621, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3320/6760 [25:28<24:15,  2.36it/s, Average Loss=0.0996, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3330/6760 [25:32<24:12,  2.36it/s, Average Loss=0.0996, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3330/6760 [25:32<24:12,  2.36it/s, Average Loss=0.0192, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3340/6760 [25:37<24:10,  2.36it/s, Average Loss=0.0192, Epoch=9]\u001b[A\n",
            "Train:  49%|████▉     | 3340/6760 [25:37<24:10,  2.36it/s, Average Loss=0.0417, Epoch=9]\u001b[A\n",
            "Train:  50%|████▉     | 3350/6760 [25:41<24:04,  2.36it/s, Average Loss=0.0417, Epoch=9]\u001b[A\n",
            "Train:  50%|████▉     | 3350/6760 [25:41<24:04,  2.36it/s, Average Loss=0.0357, Epoch=9]\u001b[A\n",
            "Train:  50%|████▉     | 3360/6760 [25:45<23:57,  2.37it/s, Average Loss=0.0357, Epoch=9]\u001b[A\n",
            "Train:  50%|████▉     | 3360/6760 [25:45<23:57,  2.37it/s, Average Loss=0.0293, Epoch=9]\u001b[A\n",
            "Train:  50%|████▉     | 3370/6760 [25:49<23:52,  2.37it/s, Average Loss=0.0293, Epoch=9]\u001b[A\n",
            "Train:  50%|████▉     | 3370/6760 [25:49<23:52,  2.37it/s, Average Loss=0.0139, Epoch=9]\u001b[A\n",
            "Train:  50%|█████     | 3380/6760 [25:54<23:48,  2.37it/s, Average Loss=0.0139, Epoch=9]\u001b[A\n",
            "Train:  50%|█████     | 3380/6760 [25:54<23:48,  2.37it/s, Average Loss=0.0333, Epoch=9]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 9, Macro F1: 0.5728818735618124, Validation Loss: 1.4743795963220818, Time per Epoch: 152.45856738090515]\n",
            "Begin Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  50%|█████     | 3390/6760 [26:12<47:52,  1.17it/s, Average Loss=0.0333, Epoch=9]\u001b[A\n",
            "Train:  50%|█████     | 3390/6760 [26:12<47:52,  1.17it/s, Average Loss=0.0922, Epoch=10]\u001b[A\n",
            "Train:  50%|█████     | 3400/6760 [26:16<40:26,  1.38it/s, Average Loss=0.0922, Epoch=10]\u001b[A\n",
            "Train:  50%|█████     | 3400/6760 [26:16<40:26,  1.38it/s, Average Loss=0.0138, Epoch=10]\u001b[A\n",
            "Train:  50%|█████     | 3410/6760 [26:20<35:14,  1.58it/s, Average Loss=0.0138, Epoch=10]\u001b[A\n",
            "Train:  50%|█████     | 3410/6760 [26:20<35:14,  1.58it/s, Average Loss=0.0482, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3420/6760 [26:25<31:40,  1.76it/s, Average Loss=0.0482, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3420/6760 [26:25<31:40,  1.76it/s, Average Loss=0.0329, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3430/6760 [26:29<29:13,  1.90it/s, Average Loss=0.0329, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3430/6760 [26:29<29:13,  1.90it/s, Average Loss=0.0302, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3440/6760 [26:33<27:24,  2.02it/s, Average Loss=0.0302, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3440/6760 [26:33<27:24,  2.02it/s, Average Loss=0.02, Epoch=10]  \u001b[A\n",
            "Train:  51%|█████     | 3450/6760 [26:37<26:07,  2.11it/s, Average Loss=0.02, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3450/6760 [26:37<26:07,  2.11it/s, Average Loss=0.0661, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3460/6760 [26:42<25:15,  2.18it/s, Average Loss=0.0661, Epoch=10]\u001b[A\n",
            "Train:  51%|█████     | 3460/6760 [26:42<25:15,  2.18it/s, Average Loss=0.0903, Epoch=10]\u001b[A\n",
            "Train:  51%|█████▏    | 3470/6760 [26:46<24:40,  2.22it/s, Average Loss=0.0903, Epoch=10]\u001b[A\n",
            "Train:  51%|█████▏    | 3470/6760 [26:46<24:40,  2.22it/s, Average Loss=0.0312, Epoch=10]\u001b[A\n",
            "Train:  51%|█████▏    | 3480/6760 [26:50<24:10,  2.26it/s, Average Loss=0.0312, Epoch=10]\u001b[A\n",
            "Train:  51%|█████▏    | 3480/6760 [26:50<24:10,  2.26it/s, Average Loss=0.00323, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3490/6760 [26:54<23:46,  2.29it/s, Average Loss=0.00323, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3490/6760 [26:54<23:46,  2.29it/s, Average Loss=0.0264, Epoch=10] \u001b[A\n",
            "Train:  52%|█████▏    | 3500/6760 [26:59<23:28,  2.31it/s, Average Loss=0.0264, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3500/6760 [26:59<23:28,  2.31it/s, Average Loss=0.0184, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3510/6760 [27:03<23:17,  2.32it/s, Average Loss=0.0184, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3510/6760 [27:03<23:17,  2.32it/s, Average Loss=0.00865, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3520/6760 [27:07<23:05,  2.34it/s, Average Loss=0.00865, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3520/6760 [27:07<23:05,  2.34it/s, Average Loss=0.00998, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3530/6760 [27:11<22:54,  2.35it/s, Average Loss=0.00998, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3530/6760 [27:11<22:54,  2.35it/s, Average Loss=0.0346, Epoch=10] \u001b[A\n",
            "Train:  52%|█████▏    | 3540/6760 [27:16<22:48,  2.35it/s, Average Loss=0.0346, Epoch=10]\u001b[A\n",
            "Train:  52%|█████▏    | 3540/6760 [27:16<22:48,  2.35it/s, Average Loss=0.0529, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3550/6760 [27:20<22:44,  2.35it/s, Average Loss=0.0529, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3550/6760 [27:20<22:44,  2.35it/s, Average Loss=0.0105, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3560/6760 [27:24<22:38,  2.36it/s, Average Loss=0.0105, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3560/6760 [27:24<22:38,  2.36it/s, Average Loss=0.0922, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3570/6760 [27:28<22:29,  2.36it/s, Average Loss=0.0922, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3570/6760 [27:28<22:29,  2.36it/s, Average Loss=0.021, Epoch=10] \u001b[A\n",
            "Train:  53%|█████▎    | 3580/6760 [27:32<22:23,  2.37it/s, Average Loss=0.021, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3580/6760 [27:32<22:23,  2.37it/s, Average Loss=0.0288, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3590/6760 [27:37<22:22,  2.36it/s, Average Loss=0.0288, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3590/6760 [27:37<22:22,  2.36it/s, Average Loss=0.0689, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3600/6760 [27:41<22:17,  2.36it/s, Average Loss=0.0689, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3600/6760 [27:41<22:17,  2.36it/s, Average Loss=0.0245, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3610/6760 [27:45<22:11,  2.37it/s, Average Loss=0.0245, Epoch=10]\u001b[A\n",
            "Train:  53%|█████▎    | 3610/6760 [27:45<22:11,  2.37it/s, Average Loss=0.0542, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▎    | 3620/6760 [27:49<22:03,  2.37it/s, Average Loss=0.0542, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▎    | 3620/6760 [27:49<22:03,  2.37it/s, Average Loss=0.00196, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▎    | 3630/6760 [27:54<22:02,  2.37it/s, Average Loss=0.00196, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▎    | 3630/6760 [27:54<22:02,  2.37it/s, Average Loss=0.0284, Epoch=10] \u001b[A\n",
            "Train:  54%|█████▍    | 3640/6760 [27:58<21:56,  2.37it/s, Average Loss=0.0284, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▍    | 3640/6760 [27:58<21:56,  2.37it/s, Average Loss=0.031, Epoch=10] \u001b[A\n",
            "Train:  54%|█████▍    | 3650/6760 [28:02<21:50,  2.37it/s, Average Loss=0.031, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▍    | 3650/6760 [28:02<21:50,  2.37it/s, Average Loss=0.0459, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▍    | 3660/6760 [28:06<21:44,  2.38it/s, Average Loss=0.0459, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▍    | 3660/6760 [28:06<21:44,  2.38it/s, Average Loss=0.0124, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▍    | 3670/6760 [28:10<21:43,  2.37it/s, Average Loss=0.0124, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▍    | 3670/6760 [28:10<21:43,  2.37it/s, Average Loss=0.0236, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▍    | 3680/6760 [28:15<21:42,  2.37it/s, Average Loss=0.0236, Epoch=10]\u001b[A\n",
            "Train:  54%|█████▍    | 3680/6760 [28:15<21:42,  2.37it/s, Average Loss=0.00297, Epoch=10]\u001b[A\n",
            "Train:  55%|█████▍    | 3690/6760 [28:19<21:35,  2.37it/s, Average Loss=0.00297, Epoch=10]\u001b[A\n",
            "Train:  55%|█████▍    | 3690/6760 [28:19<21:35,  2.37it/s, Average Loss=0.0343, Epoch=10] \u001b[A\n",
            "Train:  55%|█████▍    | 3700/6760 [28:23<21:31,  2.37it/s, Average Loss=0.0343, Epoch=10]\u001b[A\n",
            "Train:  55%|█████▍    | 3700/6760 [28:23<21:31,  2.37it/s, Average Loss=0.0343, Epoch=10]\u001b[A\n",
            "Train:  55%|█████▍    | 3710/6760 [28:27<21:29,  2.37it/s, Average Loss=0.0343, Epoch=10]\u001b[A\n",
            "Train:  55%|█████▍    | 3710/6760 [28:27<21:29,  2.37it/s, Average Loss=0.0167, Epoch=10]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 10, Macro F1: 0.5588998612832278, Validation Loss: 1.6597525740778722, Time per Epoch: 152.26336240768433]\n",
            "Begin Epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  55%|█████▌    | 3720/6760 [28:41<35:35,  1.42it/s, Average Loss=0.0167, Epoch=10]\u001b[A\n",
            "Train:  55%|█████▌    | 3720/6760 [28:41<35:35,  1.42it/s, Average Loss=0.0128, Epoch=11]\u001b[A\n",
            "Train:  55%|█████▌    | 3730/6760 [28:45<31:15,  1.62it/s, Average Loss=0.0128, Epoch=11]\u001b[A\n",
            "Train:  55%|█████▌    | 3730/6760 [28:45<31:15,  1.62it/s, Average Loss=0.115, Epoch=11] \u001b[A\n",
            "Train:  55%|█████▌    | 3740/6760 [28:49<28:14,  1.78it/s, Average Loss=0.115, Epoch=11]\u001b[A\n",
            "Train:  55%|█████▌    | 3740/6760 [28:49<28:14,  1.78it/s, Average Loss=0.00284, Epoch=11]\u001b[A\n",
            "Train:  55%|█████▌    | 3750/6760 [28:54<26:03,  1.93it/s, Average Loss=0.00284, Epoch=11]\u001b[A\n",
            "Train:  55%|█████▌    | 3750/6760 [28:54<26:03,  1.93it/s, Average Loss=0.0286, Epoch=11] \u001b[A\n",
            "Train:  56%|█████▌    | 3760/6760 [28:58<24:30,  2.04it/s, Average Loss=0.0286, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▌    | 3760/6760 [28:58<24:30,  2.04it/s, Average Loss=0.0294, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▌    | 3770/6760 [29:02<23:28,  2.12it/s, Average Loss=0.0294, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▌    | 3770/6760 [29:02<23:28,  2.12it/s, Average Loss=0.0113, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▌    | 3780/6760 [29:06<22:44,  2.18it/s, Average Loss=0.0113, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▌    | 3780/6760 [29:06<22:44,  2.18it/s, Average Loss=0.0117, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▌    | 3790/6760 [29:11<22:07,  2.24it/s, Average Loss=0.0117, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▌    | 3790/6760 [29:11<22:07,  2.24it/s, Average Loss=0.031, Epoch=11] \u001b[A\n",
            "Train:  56%|█████▌    | 3800/6760 [29:15<21:40,  2.28it/s, Average Loss=0.031, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▌    | 3800/6760 [29:15<21:40,  2.28it/s, Average Loss=0.0349, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▋    | 3810/6760 [29:19<21:22,  2.30it/s, Average Loss=0.0349, Epoch=11]\u001b[A\n",
            "Train:  56%|█████▋    | 3810/6760 [29:19<21:22,  2.30it/s, Average Loss=0.0196, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3820/6760 [29:23<21:11,  2.31it/s, Average Loss=0.0196, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3820/6760 [29:23<21:11,  2.31it/s, Average Loss=0.0337, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3830/6760 [29:28<20:58,  2.33it/s, Average Loss=0.0337, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3830/6760 [29:28<20:58,  2.33it/s, Average Loss=0.00122, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3840/6760 [29:32<20:46,  2.34it/s, Average Loss=0.00122, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3840/6760 [29:32<20:46,  2.34it/s, Average Loss=0.0525, Epoch=11] \u001b[A\n",
            "Train:  57%|█████▋    | 3850/6760 [29:36<20:38,  2.35it/s, Average Loss=0.0525, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3850/6760 [29:36<20:38,  2.35it/s, Average Loss=0.00953, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3860/6760 [29:40<20:35,  2.35it/s, Average Loss=0.00953, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3860/6760 [29:40<20:35,  2.35it/s, Average Loss=0.00488, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3870/6760 [29:45<20:28,  2.35it/s, Average Loss=0.00488, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3870/6760 [29:45<20:28,  2.35it/s, Average Loss=0.0328, Epoch=11] \u001b[A\n",
            "Train:  57%|█████▋    | 3880/6760 [29:49<20:20,  2.36it/s, Average Loss=0.0328, Epoch=11]\u001b[A\n",
            "Train:  57%|█████▋    | 3880/6760 [29:49<20:20,  2.36it/s, Average Loss=0.00489, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3890/6760 [29:53<20:13,  2.36it/s, Average Loss=0.00489, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3890/6760 [29:53<20:13,  2.36it/s, Average Loss=0.00274, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3900/6760 [29:57<20:11,  2.36it/s, Average Loss=0.00274, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3900/6760 [29:57<20:11,  2.36it/s, Average Loss=0.00137, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3910/6760 [30:01<20:07,  2.36it/s, Average Loss=0.00137, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3910/6760 [30:01<20:07,  2.36it/s, Average Loss=0.016, Epoch=11]  \u001b[A\n",
            "Train:  58%|█████▊    | 3920/6760 [30:06<20:00,  2.37it/s, Average Loss=0.016, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3920/6760 [30:06<20:00,  2.37it/s, Average Loss=0.00618, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3930/6760 [30:10<19:54,  2.37it/s, Average Loss=0.00618, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3930/6760 [30:10<19:54,  2.37it/s, Average Loss=0.00594, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3940/6760 [30:14<19:51,  2.37it/s, Average Loss=0.00594, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3940/6760 [30:14<19:51,  2.37it/s, Average Loss=0.0515, Epoch=11] \u001b[A\n",
            "Train:  58%|█████▊    | 3950/6760 [30:18<19:48,  2.36it/s, Average Loss=0.0515, Epoch=11]\u001b[A\n",
            "Train:  58%|█████▊    | 3950/6760 [30:18<19:48,  2.36it/s, Average Loss=0.00339, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▊    | 3960/6760 [30:22<19:41,  2.37it/s, Average Loss=0.00339, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▊    | 3960/6760 [30:23<19:41,  2.37it/s, Average Loss=0.00682, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▊    | 3970/6760 [30:27<19:35,  2.37it/s, Average Loss=0.00682, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▊    | 3970/6760 [30:27<19:35,  2.37it/s, Average Loss=0.0428, Epoch=11] \u001b[A\n",
            "Train:  59%|█████▉    | 3980/6760 [30:31<19:34,  2.37it/s, Average Loss=0.0428, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▉    | 3980/6760 [30:31<19:34,  2.37it/s, Average Loss=0.00185, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▉    | 3990/6760 [30:35<19:30,  2.37it/s, Average Loss=0.00185, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▉    | 3990/6760 [30:35<19:30,  2.37it/s, Average Loss=0.00811, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▉    | 4000/6760 [30:39<19:24,  2.37it/s, Average Loss=0.00811, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▉    | 4000/6760 [30:39<19:24,  2.37it/s, Average Loss=0.0398, Epoch=11] \u001b[A\n",
            "Train:  59%|█████▉    | 4010/6760 [30:44<19:19,  2.37it/s, Average Loss=0.0398, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▉    | 4010/6760 [30:44<19:19,  2.37it/s, Average Loss=0.00201, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▉    | 4020/6760 [30:48<19:16,  2.37it/s, Average Loss=0.00201, Epoch=11]\u001b[A\n",
            "Train:  59%|█████▉    | 4020/6760 [30:48<19:16,  2.37it/s, Average Loss=0.058, Epoch=11]  \u001b[A\n",
            "Train:  60%|█████▉    | 4030/6760 [30:52<19:15,  2.36it/s, Average Loss=0.058, Epoch=11]\u001b[A\n",
            "Train:  60%|█████▉    | 4030/6760 [30:52<19:15,  2.36it/s, Average Loss=0.011, Epoch=11]\u001b[A\n",
            "Train:  60%|█████▉    | 4040/6760 [30:56<19:08,  2.37it/s, Average Loss=0.011, Epoch=11]\u001b[A\n",
            "Train:  60%|█████▉    | 4040/6760 [30:56<19:08,  2.37it/s, Average Loss=0.0182, Epoch=11]\u001b[A\n",
            "Train:  60%|█████▉    | 4050/6760 [31:00<19:03,  2.37it/s, Average Loss=0.0182, Epoch=11]\u001b[A\n",
            "Train:  60%|█████▉    | 4050/6760 [31:01<19:03,  2.37it/s, Average Loss=0.0198, Epoch=11]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 11, Macro F1: 0.5607600783153369, Validation Loss: 1.854157195534817, Time per Epoch: 152.1766746044159]\n",
            "Begin Epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  60%|██████    | 4060/6760 [31:14<31:27,  1.43it/s, Average Loss=0.0198, Epoch=11]\u001b[A\n",
            "Train:  60%|██████    | 4060/6760 [31:14<31:27,  1.43it/s, Average Loss=0.0327, Epoch=12]\u001b[A\n",
            "Train:  60%|██████    | 4070/6760 [31:18<27:36,  1.62it/s, Average Loss=0.0327, Epoch=12]\u001b[A\n",
            "Train:  60%|██████    | 4070/6760 [31:18<27:36,  1.62it/s, Average Loss=0.0603, Epoch=12]\u001b[A\n",
            "Train:  60%|██████    | 4080/6760 [31:22<24:55,  1.79it/s, Average Loss=0.0603, Epoch=12]\u001b[A\n",
            "Train:  60%|██████    | 4080/6760 [31:22<24:55,  1.79it/s, Average Loss=0.0207, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4090/6760 [31:27<23:04,  1.93it/s, Average Loss=0.0207, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4090/6760 [31:27<23:04,  1.93it/s, Average Loss=0.00214, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4100/6760 [31:31<21:42,  2.04it/s, Average Loss=0.00214, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4100/6760 [31:31<21:42,  2.04it/s, Average Loss=0.0304, Epoch=12] \u001b[A\n",
            "Train:  61%|██████    | 4110/6760 [31:35<20:43,  2.13it/s, Average Loss=0.0304, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4110/6760 [31:35<20:43,  2.13it/s, Average Loss=0.00354, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4120/6760 [31:39<20:02,  2.20it/s, Average Loss=0.00354, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4120/6760 [31:39<20:02,  2.20it/s, Average Loss=0.0419, Epoch=12] \u001b[A\n",
            "Train:  61%|██████    | 4130/6760 [31:44<19:34,  2.24it/s, Average Loss=0.0419, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4130/6760 [31:44<19:34,  2.24it/s, Average Loss=0.0066, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4140/6760 [31:48<19:10,  2.28it/s, Average Loss=0.0066, Epoch=12]\u001b[A\n",
            "Train:  61%|██████    | 4140/6760 [31:48<19:10,  2.28it/s, Average Loss=0.00573, Epoch=12]\u001b[A\n",
            "Train:  61%|██████▏   | 4150/6760 [31:52<18:52,  2.30it/s, Average Loss=0.00573, Epoch=12]\u001b[A\n",
            "Train:  61%|██████▏   | 4150/6760 [31:52<18:52,  2.30it/s, Average Loss=0.00309, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4160/6760 [31:56<18:40,  2.32it/s, Average Loss=0.00309, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4160/6760 [31:56<18:40,  2.32it/s, Average Loss=0.00345, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4170/6760 [32:01<18:33,  2.33it/s, Average Loss=0.00345, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4170/6760 [32:01<18:33,  2.33it/s, Average Loss=0.0217, Epoch=12] \u001b[A\n",
            "Train:  62%|██████▏   | 4180/6760 [32:05<18:23,  2.34it/s, Average Loss=0.0217, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4180/6760 [32:05<18:23,  2.34it/s, Average Loss=0.0285, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4190/6760 [32:09<18:14,  2.35it/s, Average Loss=0.0285, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4190/6760 [32:09<18:14,  2.35it/s, Average Loss=0.0155, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4200/6760 [32:13<18:08,  2.35it/s, Average Loss=0.0155, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4200/6760 [32:13<18:08,  2.35it/s, Average Loss=0.00323, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4210/6760 [32:17<18:04,  2.35it/s, Average Loss=0.00323, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4210/6760 [32:17<18:04,  2.35it/s, Average Loss=0.0405, Epoch=12] \u001b[A\n",
            "Train:  62%|██████▏   | 4220/6760 [32:22<17:59,  2.35it/s, Average Loss=0.0405, Epoch=12]\u001b[A\n",
            "Train:  62%|██████▏   | 4220/6760 [32:22<17:59,  2.35it/s, Average Loss=0.00183, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4230/6760 [32:26<17:53,  2.36it/s, Average Loss=0.00183, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4230/6760 [32:26<17:53,  2.36it/s, Average Loss=0.00371, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4240/6760 [32:30<17:48,  2.36it/s, Average Loss=0.00371, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4240/6760 [32:30<17:48,  2.36it/s, Average Loss=0.00229, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4250/6760 [32:34<17:46,  2.35it/s, Average Loss=0.00229, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4250/6760 [32:34<17:46,  2.35it/s, Average Loss=0.00613, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4260/6760 [32:39<17:42,  2.35it/s, Average Loss=0.00613, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4260/6760 [32:39<17:42,  2.35it/s, Average Loss=0.000297, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4270/6760 [32:43<17:34,  2.36it/s, Average Loss=0.000297, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4270/6760 [32:43<17:34,  2.36it/s, Average Loss=0.0316, Epoch=12]  \u001b[A\n",
            "Train:  63%|██████▎   | 4280/6760 [32:47<17:28,  2.36it/s, Average Loss=0.0316, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4280/6760 [32:47<17:28,  2.36it/s, Average Loss=0.0356, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4290/6760 [32:51<17:26,  2.36it/s, Average Loss=0.0356, Epoch=12]\u001b[A\n",
            "Train:  63%|██████▎   | 4290/6760 [32:51<17:26,  2.36it/s, Average Loss=0.0573, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▎   | 4300/6760 [32:56<17:22,  2.36it/s, Average Loss=0.0573, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▎   | 4300/6760 [32:56<17:22,  2.36it/s, Average Loss=0.0367, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4310/6760 [33:00<17:15,  2.37it/s, Average Loss=0.0367, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4310/6760 [33:00<17:15,  2.37it/s, Average Loss=0.000433, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4320/6760 [33:04<17:10,  2.37it/s, Average Loss=0.000433, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4320/6760 [33:04<17:10,  2.37it/s, Average Loss=0.0137, Epoch=12]  \u001b[A\n",
            "Train:  64%|██████▍   | 4330/6760 [33:08<17:09,  2.36it/s, Average Loss=0.0137, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4330/6760 [33:08<17:09,  2.36it/s, Average Loss=0.000254, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4340/6760 [33:13<17:05,  2.36it/s, Average Loss=0.000254, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4340/6760 [33:13<17:05,  2.36it/s, Average Loss=0.000296, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4350/6760 [33:17<16:59,  2.36it/s, Average Loss=0.000296, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4350/6760 [33:17<16:59,  2.36it/s, Average Loss=0.00245, Epoch=12] \u001b[A\n",
            "Train:  64%|██████▍   | 4360/6760 [33:21<16:53,  2.37it/s, Average Loss=0.00245, Epoch=12]\u001b[A\n",
            "Train:  64%|██████▍   | 4360/6760 [33:21<16:53,  2.37it/s, Average Loss=0.000242, Epoch=12]\u001b[A\n",
            "Train:  65%|██████▍   | 4370/6760 [33:25<16:50,  2.36it/s, Average Loss=0.000242, Epoch=12]\u001b[A\n",
            "Train:  65%|██████▍   | 4370/6760 [33:25<16:50,  2.36it/s, Average Loss=0.0226, Epoch=12]  \u001b[A\n",
            "Train:  65%|██████▍   | 4380/6760 [33:29<16:47,  2.36it/s, Average Loss=0.0226, Epoch=12]\u001b[A\n",
            "Train:  65%|██████▍   | 4380/6760 [33:29<16:47,  2.36it/s, Average Loss=0.000229, Epoch=12]\u001b[A\n",
            "Train:  65%|██████▍   | 4390/6760 [33:34<16:42,  2.36it/s, Average Loss=0.000229, Epoch=12]\u001b[A\n",
            "Train:  65%|██████▍   | 4390/6760 [33:34<16:42,  2.36it/s, Average Loss=0.00991, Epoch=12] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 12, Macro F1: 0.5698215125078125, Validation Loss: 1.8851327133733173, Time per Epoch: 152.57233500480652]\n",
            "Begin Epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  65%|██████▌   | 4400/6760 [33:47<27:49,  1.41it/s, Average Loss=0.00991, Epoch=12]\u001b[A\n",
            "Train:  65%|██████▌   | 4400/6760 [33:47<27:49,  1.41it/s, Average Loss=0.0514, Epoch=13] \u001b[A\n",
            "Train:  65%|██████▌   | 4410/6760 [33:52<24:20,  1.61it/s, Average Loss=0.0514, Epoch=13]\u001b[A\n",
            "Train:  65%|██████▌   | 4410/6760 [33:52<24:20,  1.61it/s, Average Loss=0.00156, Epoch=13]\u001b[A\n",
            "Train:  65%|██████▌   | 4420/6760 [33:56<21:52,  1.78it/s, Average Loss=0.00156, Epoch=13]\u001b[A\n",
            "Train:  65%|██████▌   | 4420/6760 [33:56<21:52,  1.78it/s, Average Loss=0.00371, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▌   | 4430/6760 [34:00<20:11,  1.92it/s, Average Loss=0.00371, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▌   | 4430/6760 [34:00<20:11,  1.92it/s, Average Loss=0.00989, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▌   | 4440/6760 [34:04<19:00,  2.03it/s, Average Loss=0.00989, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▌   | 4440/6760 [34:04<19:00,  2.03it/s, Average Loss=0.0372, Epoch=13] \u001b[A\n",
            "Train:  66%|██████▌   | 4450/6760 [34:08<18:07,  2.13it/s, Average Loss=0.0372, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▌   | 4450/6760 [34:09<18:07,  2.13it/s, Average Loss=0.000402, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▌   | 4460/6760 [34:13<17:28,  2.19it/s, Average Loss=0.000402, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▌   | 4460/6760 [34:13<17:28,  2.19it/s, Average Loss=0.00058, Epoch=13] \u001b[A\n",
            "Train:  66%|██████▌   | 4470/6760 [34:17<17:02,  2.24it/s, Average Loss=0.00058, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▌   | 4470/6760 [34:17<17:02,  2.24it/s, Average Loss=0.0108, Epoch=13] \u001b[A\n",
            "Train:  66%|██████▋   | 4480/6760 [34:21<16:43,  2.27it/s, Average Loss=0.0108, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▋   | 4480/6760 [34:21<16:43,  2.27it/s, Average Loss=0.000933, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▋   | 4490/6760 [34:25<16:26,  2.30it/s, Average Loss=0.000933, Epoch=13]\u001b[A\n",
            "Train:  66%|██████▋   | 4490/6760 [34:25<16:26,  2.30it/s, Average Loss=0.000631, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4500/6760 [34:30<16:13,  2.32it/s, Average Loss=0.000631, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4500/6760 [34:30<16:13,  2.32it/s, Average Loss=0.00372, Epoch=13] \u001b[A\n",
            "Train:  67%|██████▋   | 4510/6760 [34:34<16:05,  2.33it/s, Average Loss=0.00372, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4510/6760 [34:34<16:05,  2.33it/s, Average Loss=0.00131, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4520/6760 [34:38<15:58,  2.34it/s, Average Loss=0.00131, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4520/6760 [34:38<15:58,  2.34it/s, Average Loss=0.0236, Epoch=13] \u001b[A\n",
            "Train:  67%|██████▋   | 4530/6760 [34:42<15:50,  2.35it/s, Average Loss=0.0236, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4530/6760 [34:42<15:50,  2.35it/s, Average Loss=0.0161, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4540/6760 [34:47<15:42,  2.36it/s, Average Loss=0.0161, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4540/6760 [34:47<15:42,  2.36it/s, Average Loss=0.000142, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4550/6760 [34:51<15:37,  2.36it/s, Average Loss=0.000142, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4550/6760 [34:51<15:37,  2.36it/s, Average Loss=0.0321, Epoch=13]  \u001b[A\n",
            "Train:  67%|██████▋   | 4560/6760 [34:55<15:34,  2.36it/s, Average Loss=0.0321, Epoch=13]\u001b[A\n",
            "Train:  67%|██████▋   | 4560/6760 [34:55<15:34,  2.36it/s, Average Loss=0.00117, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4570/6760 [34:59<15:29,  2.36it/s, Average Loss=0.00117, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4570/6760 [34:59<15:29,  2.36it/s, Average Loss=0.00323, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4580/6760 [35:04<15:23,  2.36it/s, Average Loss=0.00323, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4580/6760 [35:04<15:23,  2.36it/s, Average Loss=0.00257, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4590/6760 [35:08<15:18,  2.36it/s, Average Loss=0.00257, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4590/6760 [35:08<15:18,  2.36it/s, Average Loss=0.002, Epoch=13]  \u001b[A\n",
            "Train:  68%|██████▊   | 4600/6760 [35:12<15:14,  2.36it/s, Average Loss=0.002, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4600/6760 [35:12<15:14,  2.36it/s, Average Loss=0.0294, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4610/6760 [35:16<15:11,  2.36it/s, Average Loss=0.0294, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4610/6760 [35:16<15:11,  2.36it/s, Average Loss=0.0313, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4620/6760 [35:20<15:05,  2.36it/s, Average Loss=0.0313, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4620/6760 [35:20<15:05,  2.36it/s, Average Loss=0.0255, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4630/6760 [35:25<15:01,  2.36it/s, Average Loss=0.0255, Epoch=13]\u001b[A\n",
            "Train:  68%|██████▊   | 4630/6760 [35:25<15:01,  2.36it/s, Average Loss=0.0158, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▊   | 4640/6760 [35:29<14:59,  2.36it/s, Average Loss=0.0158, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▊   | 4640/6760 [35:29<14:59,  2.36it/s, Average Loss=0.000177, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4650/6760 [35:33<14:55,  2.36it/s, Average Loss=0.000177, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4650/6760 [35:33<14:55,  2.36it/s, Average Loss=0.0174, Epoch=13]  \u001b[A\n",
            "Train:  69%|██████▉   | 4660/6760 [35:37<14:49,  2.36it/s, Average Loss=0.0174, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4660/6760 [35:37<14:49,  2.36it/s, Average Loss=0.00231, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4670/6760 [35:42<14:45,  2.36it/s, Average Loss=0.00231, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4670/6760 [35:42<14:45,  2.36it/s, Average Loss=0.000718, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4680/6760 [35:46<14:42,  2.36it/s, Average Loss=0.000718, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4680/6760 [35:46<14:42,  2.36it/s, Average Loss=0.000342, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4690/6760 [35:50<14:38,  2.36it/s, Average Loss=0.000342, Epoch=13]\u001b[A\n",
            "Train:  69%|██████▉   | 4690/6760 [35:50<14:38,  2.36it/s, Average Loss=0.000162, Epoch=13]\u001b[A\n",
            "Train:  70%|██████▉   | 4700/6760 [35:54<14:32,  2.36it/s, Average Loss=0.000162, Epoch=13]\u001b[A\n",
            "Train:  70%|██████▉   | 4700/6760 [35:54<14:32,  2.36it/s, Average Loss=0.000347, Epoch=13]\u001b[A\n",
            "Train:  70%|██████▉   | 4710/6760 [35:59<14:27,  2.36it/s, Average Loss=0.000347, Epoch=13]\u001b[A\n",
            "Train:  70%|██████▉   | 4710/6760 [35:59<14:27,  2.36it/s, Average Loss=0.000214, Epoch=13]\u001b[A\n",
            "Train:  70%|██████▉   | 4720/6760 [36:03<14:24,  2.36it/s, Average Loss=0.000214, Epoch=13]\u001b[A\n",
            "Train:  70%|██████▉   | 4720/6760 [36:03<14:24,  2.36it/s, Average Loss=0.00648, Epoch=13] \u001b[A\n",
            "Train:  70%|██████▉   | 4730/6760 [36:07<14:21,  2.36it/s, Average Loss=0.00648, Epoch=13]\u001b[A\n",
            "Train:  70%|██████▉   | 4730/6760 [36:07<14:21,  2.36it/s, Average Loss=0.0306, Epoch=13] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 13, Macro F1: 0.5641442223210607, Validation Loss: 1.870602618816287, Time per Epoch: 152.46225571632385]\n",
            "Begin Epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train:  70%|███████   | 4740/6760 [36:21<23:44,  1.42it/s, Average Loss=0.0306, Epoch=13]\u001b[A\n",
            "Train:  70%|███████   | 4740/6760 [36:21<23:44,  1.42it/s, Average Loss=0.0401, Epoch=14]\u001b[A\n",
            "Train:  70%|███████   | 4750/6760 [36:25<20:48,  1.61it/s, Average Loss=0.0401, Epoch=14]\u001b[A\n",
            "Train:  70%|███████   | 4750/6760 [36:25<20:48,  1.61it/s, Average Loss=0.000494, Epoch=14]\u001b[A\n",
            "Train:  70%|███████   | 4760/6760 [36:29<18:42,  1.78it/s, Average Loss=0.000494, Epoch=14]\u001b[A\n",
            "Train:  70%|███████   | 4760/6760 [36:29<18:42,  1.78it/s, Average Loss=0.00335, Epoch=14] \u001b[A\n",
            "Train:  71%|███████   | 4770/6760 [36:33<17:12,  1.93it/s, Average Loss=0.00335, Epoch=14]\u001b[A\n",
            "Train:  71%|███████   | 4770/6760 [36:33<17:12,  1.93it/s, Average Loss=0.0323, Epoch=14] \u001b[A\n",
            "Train:  71%|███████   | 4780/6760 [36:38<16:11,  2.04it/s, Average Loss=0.0323, Epoch=14]\u001b[A\n",
            "Train:  71%|███████   | 4780/6760 [36:38<16:11,  2.04it/s, Average Loss=0.000422, Epoch=14]\u001b[A\n",
            "Train:  71%|███████   | 4790/6760 [36:42<15:28,  2.12it/s, Average Loss=0.000422, Epoch=14]\u001b[A\n",
            "Train:  71%|███████   | 4790/6760 [36:42<15:28,  2.12it/s, Average Loss=0.000362, Epoch=14]\u001b[A\n",
            "Train:  71%|███████   | 4800/6760 [36:46<14:53,  2.19it/s, Average Loss=0.000362, Epoch=14]\u001b[A\n",
            "Train:  71%|███████   | 4800/6760 [36:46<14:53,  2.19it/s, Average Loss=0.00505, Epoch=14] \u001b[A\n",
            "Train:  71%|███████   | 4810/6760 [36:50<14:28,  2.24it/s, Average Loss=0.00505, Epoch=14]\u001b[A\n",
            "Train:  71%|███████   | 4810/6760 [36:50<14:28,  2.24it/s, Average Loss=0.000628, Epoch=14]\u001b[A\n",
            "Train:  71%|███████▏  | 4820/6760 [36:55<14:12,  2.28it/s, Average Loss=0.000628, Epoch=14]\u001b[A\n",
            "Train:  71%|███████▏  | 4820/6760 [36:55<14:12,  2.28it/s, Average Loss=0.000893, Epoch=14]\u001b[A\n",
            "Train:  71%|███████▏  | 4830/6760 [36:59<13:59,  2.30it/s, Average Loss=0.000893, Epoch=14]\u001b[A\n",
            "Train:  71%|███████▏  | 4830/6760 [36:59<13:59,  2.30it/s, Average Loss=0.000181, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4840/6760 [37:03<13:47,  2.32it/s, Average Loss=0.000181, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4840/6760 [37:03<13:47,  2.32it/s, Average Loss=0.0346, Epoch=14]  \u001b[A\n",
            "Train:  72%|███████▏  | 4850/6760 [37:07<13:37,  2.34it/s, Average Loss=0.0346, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4850/6760 [37:07<13:37,  2.34it/s, Average Loss=0.000361, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4860/6760 [37:11<13:31,  2.34it/s, Average Loss=0.000361, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4860/6760 [37:11<13:31,  2.34it/s, Average Loss=0.00644, Epoch=14] \u001b[A\n",
            "Train:  72%|███████▏  | 4870/6760 [37:16<13:26,  2.34it/s, Average Loss=0.00644, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4870/6760 [37:16<13:26,  2.34it/s, Average Loss=0.0021, Epoch=14] \u001b[A\n",
            "Train:  72%|███████▏  | 4880/6760 [37:20<13:18,  2.35it/s, Average Loss=0.0021, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4880/6760 [37:20<13:18,  2.35it/s, Average Loss=0.000812, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4890/6760 [37:24<13:12,  2.36it/s, Average Loss=0.000812, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4890/6760 [37:24<13:12,  2.36it/s, Average Loss=0.0135, Epoch=14]  \u001b[A\n",
            "Train:  72%|███████▏  | 4900/6760 [37:28<13:08,  2.36it/s, Average Loss=0.0135, Epoch=14]\u001b[A\n",
            "Train:  72%|███████▏  | 4900/6760 [37:28<13:08,  2.36it/s, Average Loss=0.00971, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4910/6760 [37:33<13:05,  2.35it/s, Average Loss=0.00971, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4910/6760 [37:33<13:05,  2.35it/s, Average Loss=0.00021, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4920/6760 [37:37<12:59,  2.36it/s, Average Loss=0.00021, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4920/6760 [37:37<12:59,  2.36it/s, Average Loss=0.000454, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4930/6760 [37:41<12:53,  2.37it/s, Average Loss=0.000454, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4930/6760 [37:41<12:53,  2.37it/s, Average Loss=0.000447, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4940/6760 [37:45<12:49,  2.37it/s, Average Loss=0.000447, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4940/6760 [37:45<12:49,  2.37it/s, Average Loss=0.000469, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4950/6760 [37:50<12:46,  2.36it/s, Average Loss=0.000469, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4950/6760 [37:50<12:46,  2.36it/s, Average Loss=0.0248, Epoch=14]  \u001b[A\n",
            "Train:  73%|███████▎  | 4960/6760 [37:54<12:41,  2.36it/s, Average Loss=0.0248, Epoch=14]\u001b[A\n",
            "Train:  73%|███████▎  | 4960/6760 [37:54<12:41,  2.36it/s, Average Loss=0.000333, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▎  | 4970/6760 [37:58<12:36,  2.37it/s, Average Loss=0.000333, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▎  | 4970/6760 [37:58<12:36,  2.37it/s, Average Loss=0.00172, Epoch=14] \u001b[A\n",
            "Train:  74%|███████▎  | 4980/6760 [38:02<12:31,  2.37it/s, Average Loss=0.00172, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▎  | 4980/6760 [38:02<12:31,  2.37it/s, Average Loss=0.000157, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 4990/6760 [38:06<12:28,  2.37it/s, Average Loss=0.000157, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 4990/6760 [38:06<12:28,  2.37it/s, Average Loss=0.000136, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 5000/6760 [38:11<12:24,  2.36it/s, Average Loss=0.000136, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 5000/6760 [38:11<12:24,  2.36it/s, Average Loss=0.000192, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 5010/6760 [38:15<12:19,  2.37it/s, Average Loss=0.000192, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 5010/6760 [38:15<12:19,  2.37it/s, Average Loss=0.00264, Epoch=14] \u001b[A\n",
            "Train:  74%|███████▍  | 5020/6760 [38:19<12:13,  2.37it/s, Average Loss=0.00264, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 5020/6760 [38:19<12:13,  2.37it/s, Average Loss=0.000153, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 5030/6760 [38:23<12:11,  2.37it/s, Average Loss=0.000153, Epoch=14]\u001b[A\n",
            "Train:  74%|███████▍  | 5030/6760 [38:23<12:11,  2.37it/s, Average Loss=0.000153, Epoch=14]\u001b[A\n",
            "Train:  75%|███████▍  | 5040/6760 [38:28<12:07,  2.36it/s, Average Loss=0.000153, Epoch=14]\u001b[A\n",
            "Train:  75%|███████▍  | 5040/6760 [38:28<12:07,  2.36it/s, Average Loss=0.00297, Epoch=14] \u001b[A\n",
            "Train:  75%|███████▍  | 5050/6760 [38:32<12:02,  2.37it/s, Average Loss=0.00297, Epoch=14]\u001b[A\n",
            "Train:  75%|███████▍  | 5050/6760 [38:32<12:02,  2.37it/s, Average Loss=0.00156, Epoch=14]\u001b[A\n",
            "Train:  75%|███████▍  | 5060/6760 [38:36<11:57,  2.37it/s, Average Loss=0.00156, Epoch=14]\u001b[A\n",
            "Train:  75%|███████▍  | 5060/6760 [38:36<11:57,  2.37it/s, Average Loss=0.000226, Epoch=14]\u001b[A\n",
            "Train:  75%|███████▌  | 5070/6760 [38:40<11:54,  2.37it/s, Average Loss=0.000226, Epoch=14]\u001b[A\n",
            "Train:  75%|███████▌  | 5070/6760 [38:40<11:54,  2.37it/s, Average Loss=0.00195, Epoch=14] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: [Epoch: 14, Macro F1: 0.5596018680583923, Validation Loss: 2.0179991749830024, Time per Epoch: 152.2863471508026]\n",
            "Training stopped early at Epoch: 14\n"
          ]
        }
      ],
      "source": [
        "pbar = tqdm(total=num_training_steps, desc=\"Train\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Begin Epoch {epoch}\")\n",
        "    epoch_start_time = time.time()\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "        actual_step += 1\n",
        "\n",
        "        if actual_step % gradient_accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            glb_step += 1\n",
        "\n",
        "            if glb_step % pbar_update_freq == 0:\n",
        "                aveloss = (tot_loss - log_loss)/pbar_update_freq\n",
        "                pbar.update(pbar_update_freq)\n",
        "                pbar.set_postfix({'Average Loss': aveloss, \"Epoch\": epoch})\n",
        "                log_loss = tot_loss\n",
        "\n",
        "            if optimizer is not None:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "        if glb_step > num_training_steps:\n",
        "            leave_training = True\n",
        "            break\n",
        "\n",
        "    val_acc, val_loss = calculate_loss_and_f1(model, validation_loader)\n",
        "    epoch_traces.append(epoch)\n",
        "    acc_traces.append(val_acc)\n",
        "    validation_loss_traces.append(val_loss)\n",
        "    print(\"Validation: [Epoch: {}, Macro F1: {}, Validation Loss: {}, Time per Epoch: {}]\".format(epoch, val_acc, val_loss, time.time()-epoch_start_time), flush=True)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        torch.save(model.state_dict(),f\"best_model.ckpt\")\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "    elif (epoch - best_epoch) >= early_stop_epoch_thresh:\n",
        "        print(\"Training stopped early at Epoch: %d\" % epoch)\n",
        "        break  # Terminate the training loop\n",
        "\n",
        "    if leave_training:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "hzB2Rn4-GXSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbdcaac-c688-4150-f5bd-741b46b8bb54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(f\"best_model.ckpt\"))\n",
        "model.cuda()\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "9U5FrKYZGZnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb3c595-e364-4aad-f47e-c51736b55a23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7477810650887574,\n",
              " 'precision': 0.7535532407483058,\n",
              " 'recall': 0.7477810650887574,\n",
              " 'f1': 0.7500756799333012,\n",
              " 'macro_precision': 0.5678811198252267,\n",
              " 'macro_recall': 0.5787958132484997,\n",
              " 'macro_f1': 0.5728818735618124}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "evaluate(model, validation_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "zE3UzWNHGcnM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "5797ed6e-a247-43a8-faba-294c9fc3d819"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(len(epoch_traces)), acc_traces)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Macro F1-Score')\n",
        "plt.title('Epoch vs Validation Macro F1-Score')\n",
        "plt.xticks(range(len(epoch_traces)), epoch_traces)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "y1y6enP6GewI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "3e15db6c-1f4b-4970-cc57-7c42c60bf5f9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(len(epoch_traces)), validation_loss_traces)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Epoch vs Validation Loss')\n",
        "plt.xticks(range(len(epoch_traces)), epoch_traces)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "q9Aw8Z2vGgyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aac435e-7c0a-4825-f5a2-6f392f48612b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7869822485207101,\n",
              " 'precision': 0.7909477009918529,\n",
              " 'recall': 0.7869822485207101,\n",
              " 'f1': 0.788464027786074,\n",
              " 'macro_precision': 0.5602566773437976,\n",
              " 'macro_recall': 0.572738895693988,\n",
              " 'macro_f1': 0.5659909328485181}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "dNMicbCTGi7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175176e8-156d-428f-9754-d3186cc8bdd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7911522633744856,\n",
              " 'precision': 0.7972649088195254,\n",
              " 'recall': 0.7911522633744856,\n",
              " 'f1': 0.7936530060160437,\n",
              " 'macro_precision': 0.5341119270698207,\n",
              " 'macro_recall': 0.5557662925403488,\n",
              " 'macro_f1': 0.5437776501811568}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "evaluate(model, test_loader_latin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "C0_4LABCGk91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e19bbf4-6163-4846-8fd1-1e753b0447b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7771739130434783,\n",
              " 'precision': 0.7873515165494176,\n",
              " 'recall': 0.7771739130434783,\n",
              " 'f1': 0.7756810160065651,\n",
              " 'macro_precision': 0.8329208507010232,\n",
              " 'macro_recall': 0.6574192176870748,\n",
              " 'macro_f1': 0.7135907517212492}"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "evaluate(model, test_loader_Sinhala)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "5xfCBwDKGm9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d19e6d-f9b4-4561-92df-678682cb8f20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7743589743589744,\n",
              " 'precision': 0.7856015779092701,\n",
              " 'recall': 0.7743589743589744,\n",
              " 'f1': 0.7767365967365968,\n",
              " 'macro_precision': 0.541826923076923,\n",
              " 'macro_recall': 0.550666144200627,\n",
              " 'macro_f1': 0.5443181818181818}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "evaluate(model, test_loader_Mixed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZjKnNRK_dDo"
      },
      "execution_count": 102,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}